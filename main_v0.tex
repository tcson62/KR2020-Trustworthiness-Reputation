%
%%%% kr.tex

\typeout{KR--20 Instructions for Authors}

% These are the instructions for authors for KR-20.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{kr}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
%\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{algorithmicx}
\urlstyle{same}

% Add additional packages here, but check
% the list of disallowed packages
% (including, but not limited to
% authblk, caption, CJK, float, fullpage, geometry,
% hyperref, layout, nameref, natbib, savetrees,
% setspace, titlesec, tocbibind, ulem)
% and illegal commands provided in the
% common formatting errors document
% included in the Author Kit before doing so.
%
% PDFINFO
% You are required to complete the following
% for pass-through to the PDF.
% No LaTeX commands of any kind may be
% entered. The parentheses and spaces
% are an integral part of the
% pdfinfo script and must not be removed.
%
%\pdfinfo{
%/Title (Type Your Paper Title Here in Mixed Case)
%/Author (John Doe, Jane Doe)
%/Keywords (Input your keywords in this optional area)
%}
%
% Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{0}




\usepackage{mathptmx}

\usepackage{amsfonts, amsmath, amssymb, mathrsfs}
\usepackage{times, helvet, courier}
\usepackage{graphicx, epsfig, epstopdf}
\usepackage{wrapfig, caption, multirow, url}
\usepackage[ruled,linesnumbered,noresetcount,vlined]{algorithm2e}
\usepackage{stmaryrd}
\usepackage{url}
%\usepackage{alltt}
\usepackage{color}
\usepackage{picinpar}
 
\newboolean{includeMemo}
\setboolean{includeMemo}{true} % boolvar=true or false

%%%%%%\usepackage{tikz}
%%%%%%\usetikzlibrary{arrows,chains,positioning,automata,decorations,shapes,calc,matrix,fit}
%\usepackage{smartdiagram}

%\usepackage[nodisplayskipstretch]{setspace}
%\setstretch{1}
\iffalse 
\setlength{\belowdisplayskip}{0pt} 
\setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} 
\setlength{\abovedisplayshortskip}{0pt}


\setlength{\abovecaptionskip}{1pt} 
\setlength{\belowcaptionskip}{-1pt} 

\setlength{\dbltextfloatsep}{-1pt}  
\setlength{\dblfloatsep}{-1pt}
\fi

\newcommand{\memo}[1]{
  \ifthenelse {\boolean{includeMemo}}{\medskip\noindent\fbox{\begin{minipage}[b]{\dimexpr\linewidth-1em}#1\end{minipage}}\medskip\newline} 
}
 


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}

\newtheorem{example}{Example}%[section]
\newtheorem{exercise}{Exercise}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}


\def\naf{\: {not} \:}
\def\cra{\stackrel{_+}{\leftarrow}}
\def\executable{\:\: \texttt{executable if} \:\:}
\newcommand{\causesif}[1]{\:\: \texttt{causes} \:\: {#1} \:\: \texttt{if} \:\:}
\newcommand{\ifthen}{\:\:\texttt{if} \:\:}

\allowdisplaybreaks

%\jdate{April 2016}
%\pubyear{2016}
%\pagerange{\pageref{firstpage}--\pageref{lastpage}}
%\doi{S1471068401001193}

\usepackage{listings}

\definecolor{darkgray}{rgb}{0.66, 0.66, 0.66} % needed for a clean compile
\lstdefinelanguage{clingo}{
  keywordstyle=[1]\usefont{OT1}{cmtt}{m}{n},%
  keywordstyle=[2]\textbf,%
  keywordstyle=[3]\usefont{OT1}{cmtt}{m}{n},%\textit
  alsoletter={\#,\&},%
  keywords=[1]{not,from,import,exists,if,else,return,while,
                break,and,or,for,in,del,and,class,subClass,concern,aspect,subCo,prop,rdf,cpsf,addBy,suppBy,neg,d,relation,holds,h,obs,action,fluent,occurs,req,group,leadTo,use,deg,comp,order,hSubCo,llh_sat_sub,llh_sat,llh_sat_sub_aux,step,deg_pos,nAllPosCon,nActPosCon,possImpactsPos,scoreLoS,wBus,wHum,wTru,wFun,wTim,wBou,wLif,wCom,wDat,last,addFun,func_dec,formulas,sat_formulas,possImpactsNeg,identify_conflict},%
  keywords=[2]{\#const,\#show,\#minimize,\#maximize,\#base,\#theory,\#count,\#external,\#program,\#script,\#end,\#heuristic,\#edge,\#project,\#show},%
  keywords=[3]{&,&dom,&sum,&diff,&show,&minimize},%
  morecomment=[l]{\#\ },%
  morecomment=[l]{\%\ },%
  commentstyle={\color{darkgray}}%
}
\lstset{numbers=left,numberblanklines=false,basicstyle=\ttfamily\small,language=clingo}
\lstset{escapeinside={*@}{@*}}
 


%Marcello Balduccini, Michael Gelfond, Enrico Pontelli, Tran Cao Son: An Answer Set Programming Framework for Reasoning about Agents' Beliefs and Truthfulness of Statements


% Title and Author Information Must Immediately Follow
% the pdfinfo within the preamble
%
\title{An Answer Set Programming Framework for Reasoning about Agents' Beliefs and Truthfulness of Statements} %\\
\author{Marcello Balduccini$^1$ \And 
Michael Gelfond$^2$ \And
Enrico Pontelli$^3$ \and Tran Cao Son$^3$ 
\affiliations
$^1$Saint Joseph's University \and 
$^2$Texas Tech University\and 
$^3$New Mexico State University
\emails
   mbalducc@sju.edu,   
   michael.gelfond@ttu.edu,
\{epontell, tson\}@cs.nmsu.edu 
} %\\

%<<<<<<<<<< AAAI format


 
\begin{document}  

\label{firstpage}


\maketitle

\begin{abstract}
The paper proposes a framework for capturing how an agent’s beliefs evolve over time  in response to observations and for answering the question of whether  statements 
made by a third party can be believed. The basic components of the framework are a formalism for 
reasoning about actions, changes, and observations and a formalism for default reasoning. 
The paper describes a concrete implementation that leverages answer set programming for   
determining the evolution of an agent's ``belief state'', based on observations, knowledge about the effects of actions, and a theory about how these influence an agent's beliefs. 
The beliefs are then used to assess whether statements made by a third party can be accepted as truthful. The paper investigates an application of the 
proposed framework in the detection of
 man-in-the-middle attacks targeting computers and cyber-physical systems. 
Finally, we briefly 
discuss related work   and   possible extensions. 
\end{abstract}

% \begin{keywords}
%        Agents, ASP, Reasoning, Knowledge
%  \end{keywords}


 %
\section{Introduction} 

Artificial Intelligence research, in the area of knowledge-based and intelligent agents, has been progressing at
a rapid pace, making it possible to develop agents that can assist and/or replace humans in 
several tasks. Organizations  use  web-bots for interacting with clients in various capacities, e.g., in providing information about the company or making offers. This trend continues to be fueled by the ubiquitous use of online resources. 

Unfortunately, not every business on the Internet is as 
honest as one would hope. Stories about businesses that cheat people of goods or services, or falsely advertise their services are not uncommon. Indeed, lying and misrepresentations are more widespread in online negotiations than 
offline ones \cite{elie}. Dishonesty is also present in
the news media, leading to the dissemination of untruthful stories  (e.g., the `Phuc Dat Bich' story\footnote{ 
{\tiny
\url{http://gizmodo.com/phuc-dat-bich-is-a-massive-phucking-faker-1744588099}
}} reported by BBC and Sky News). Another example is the case of a hotel reviewer with the alias {\em ``AmishBoy''} who gave high ratings to different hotels in the same day \cite{MinnichCMLF15}---thus suggesting  that his statements may be untrustworthy.  

The concern about misinformation, disinformation, or dishonesty is also reflected in the 
development of resources that allow people to rate companies (e.g., \url{expedia.com}, \url{yelp.com},  \emph{Angie's List}) or defend the reputation of a company or an entity (e.g., \url{reputation.com}) or check for the accuracy of some information disseminated by a person or a new outlet (e.g., \url{factcheck.org}). All these aim at providing users with tools for checking the trustworthiness of a piece of information. 

The above development highlights the problem of  \emph{understanding whether statements made by an agent can be accepted as truthful with respect to a context composed of observations,knowledge about default behavior, and corresponding beliefs.} This problem is not new to human. It is indeed as old as the existence of human on the earth. Yet, the amount of information that a human is receiving everyday has gone up significantly since the earth becomes flat, with the introduction of the Internet, and the advances in mobile technologies. This requires, ideally, tools that automatically filter out untrustworthy information for an user. The development of such a tool, on the other hand, requires the understanding of the reasoning process that helps a human to conclude that a piece of information receiving from a third party is true or false.   
 
% \memo{I modified this part - up to this point: dividing into paragraphs, adding the last one} 
  

%\begin{example}
%The Gizmodo website carried the following story:\footnote{ 
%{\tiny
%\url{http://gizmodo.com/phuc-dat-bich-is-a-massive-phucking-faker-}\\
%\url{1744588099}
%}}
%A man named Tin Le from Australia managed to fool the world's media by posting his screenshot of a passport with the name ``Phuc Dat Bich'' and claiming that he was denied access to a social network because of his name. It turned out to be a hoax. Yet, before he acknowledged that it was a hoax, BBC and Sky News  reported the story as true. 
%Observe that the story was reported as a true story  even if requests for interview had been denied. 
%\end{example} 
%


%The above example displays a typical situation in which a piece of information is disseminated as true but later found  to be false. It would have been better if the media had verified the truthfulness of the statement before disseminating it. In fact, if they had  applied some common-sense, by considering that  ``{\em normally, a statement cannot be considered as true if it is not verified}'',  then they would  have not reported the hoax. The example suggests the need to develop systems capable of determining whether  a given statement is truthful or not. 




In this paper, we are interested in reasoning about the ``belief state'' of an agent and what that tells us about the truthfulness of statements made by a third party. We assume that we can observe the world as well as other agents' actions. The basis for our judgments will be composed of our observations, performed on
a linear time line, along with  commonsense knowledge about the world and about how observations and knowledge influence an agent's beliefs. We assume that observations are \emph{true} at the time they are made, and will stay true until additional pieces of information prove otherwise. The approach is designed to reflect the fact that an agent's beliefs are subjective -- they might not correspond to the ground truth and could change over time. This is because judgments are often made in the presence of incomplete information. This makes reasoning about beliefs and truthfulness of statements \emph{non-monotonic}. Note that, in this paper, judgments about a statement are made  \emph{independently} of whether or not we trust the agent from whom the statement originated. As such, our study in this paper differs from the 
extensive literature about trust models between agents  (see, e.g., \cite{ArtzG07,SabaterS05}). We will elaborate further on this issue in the related work section. 

Consider the following motivating example.
\begin{example} \label{ex1}
When we first met at an event (time $t_0$), John stated that he comes from a poor family. 
Later, we  learn that John attends a private college, renowned for  its expensive tuition (time $t_1$). Later yet, we learn 
that John attends the college thanks to a  scholarship awarded  due to his financial hardship (time $t_2$).

This story spans over three time instants: $t_0$, $t_1$, and $t_2$. During the story, we learn (through observations) some facts, which  affect our beliefs about the world as follows:    

\begin{itemize}

\item Time $t_0$: John makes the statement that his family is poor (property $poor$ is true). No observations are available to us and thus we do not know if John's family is indeed poor or not. We have no reasons to conclude that John's statement is not truthful. 

\item Time $t_1$:  We observe that John attends an expensive college (property $in\_college$ is true). {\emph{Normally}, we believe that someone attending an expensive college is from a rich family} (default $d_1$). Hence, our belief at this point is that John is rich. This prevents us from accepting John's statement as truthful. We indicate that  the default $d_1$ is the reason to draw such conclusion. 

\item Time $t_2$: We observe that John has a need-based scholarship   (property $has\_scholarship$ is true). {It is our belief that a student's need-based scholarship is \emph{usually} derived from the family's financial situation} (default $d_2$). Because both defaults are applicable and in conflict with each other, we believe neither that John's family is rich, nor that it is poor. This allows us to withdraw our previous conclusion that John's statement could not be accepted as truthful.  

\item The situation might be different if we had a preference between our defaults. For instance, if we were inclined to favor $d_2$ over $d_1$, then we would believe that John's family is poor. This may strengthen our conclusion that John's statement can be accepted as truthful. 
% 
\end{itemize}  
\end{example}  

The main contributions of this paper can be summarized as follows:
\begin{enumerate}
\item The formalization of an abstract model to 
represent and reason about the evolution of an agent's beliefs over time and to draw conclusions about whether third-party statements can be accepted as truthful;   
\item A concrete realization of the model using
Answer Set Programming; and
\item A demonstration of the proposed framework on an important problem in the
        area of  cyber security. 
\end{enumerate}
%The paper illustrates  how, starting
% from ({\em (i)}) a collection of observations about the (possibly evolving) state of the
% world, {\em (ii)}
% observations about (some of) the  actions performed by the agent, and 
%{\em (iii)}
% the observer's own bias about the \emph{normal} behavior of agents,
% an observer can draw conclusions about the truthfulness of statements made by
% the observed agent. 
 We illustrate the framework 
 using examples and discuss possible extensions that need to be considered. 
%We also discuss a potential application of the proposed framework. 
 


 \iffalse 



For a rule $r$ of form (\ref{lprule1}), $H(r)$ and 
$B(r)$ denote the left and right hand side 
of $\leftarrow$, respectively; 
$head(r)$ denotes $\{c_1, \ldots,  c_k\}$; and 
$pos(r)$ and $neg(r)$ denote $\{a_1,\ldots,a_m\}$ and 
$\{a_{m+1},\ldots, a_n\}$.  
For a program $\Pi$, $lit(\Pi)$ denotes the set of literals occurring in $\Pi$. 

Consider a set of ground literals $X$. 
$X$ is \emph{consistent} if there exists no atom $a$ such that 
both $a$ and $\neg a$ belong to $X$. The
body of a rule $r$ of the form (\ref{lprule1}) is
{\em satisfied} by $X$ if $neg(r) \cap X = \emptyset$
and $pos(r) \subseteq X$. A rule of form
(\ref{lprule1}) with nonempty head 
is satisfied by $X$ if either its body is not
satisfied by $X$ or $head(r) \cap X \ne \emptyset$. 
A constraint is {\em satisfied} by $X$ 
if its body is not satisfied by $X$. 

For a consistent set of ground literals $S$ and a 
program $\Pi$, the {\em reduct} 
of $\Pi$ w.r.t. $S$, denoted by $\Pi^S$, is the program
obtained from  $\Pi$ by deleting
{\bf (i)} each rule that has a naf-literal {\em not} $a$ in its body with
$a \in S$, and
{\bf {ii}} all naf-literals in the bodies of the remaining rules.

$S$ is an \emph{answer set (or a stable model)} of $\Pi$ \cite{GelfondL90} if it
satisfies the following conditions: 
{\bf (i)}  If $\Pi$ does not contain any naf-literal
(i.e., $m=n$ in every rule of $\Pi$) then $S$ is a minimal 
consistent set of literals that satisfies all the rules in $\Pi$; and
{\bf (ii)} If $\Pi$ does contain some naf-literal
($m < n$ in some rules of $\Pi$), then $S$ is an answer set of
$\Pi$ if $S$ is the answer set of $\Pi^S$. Note that $\Pi^S$ does
not contain naf-literals; hence its answer set is defined in the first
item.
A program $\Pi$ is said to be \emph{consistent} if it has an
 answer set. Otherwise, it is inconsistent. 


To increase the expressiveness of ASP and  
simplify its use in applications, the language
 has been extended with several constructs such as 
\begin{itemize} 
\item Weight constraint atoms (e.g., \cite{nie99b})
of the form: 
%%
\begin{equation}
\label{weight}
\begin{array}{l}
  l\: [a_1 = w_1, \ldots, a_n = w_n, \\
  \hspace*{.4in}{\naf} b_{n+1} = w_{n+1},\ldots,b_{n+k} = w_{n+k} ]\: u 
\end{array}
\end{equation}
%
where $a_i$ and $b_j$ are literals and $l$, $u$, and $w_j$'s are integers,
$l \le u$. 
%Such an atom is satisfied in a set of literal $X$ 
%if $l \le \sum_{a_i \in X} w_i + \sum_{b_{n+k} \not\in X} w_{n+k} \le u$. 
 

\item Aggregates atoms (e.g, \cite{faberLP04,PelovDB04,SonP05})
 of the form:
%%
\begin{equation}
\label{weight}
f(S) \: op \: v
\end{equation} 
where $S$ is a set-literal, $f \in \{\textnormal{\sc Sum}, \textnormal{\sc Count},
\textnormal{\sc Max}, \textnormal{\sc Min} \}$, 
$op \in \{>, <, \ge, \le, =\}$, 
and $v$ is a number; a set-literal is of the form   
({\em i}) $\{\vec{X} \mid p(\vec{W})\}$ where $\vec{X}$ is a vector of variables, $\vec{W}$ is vector of parameters and constants such that each variable in $\vec{X}$ also occurs in $\vec{W}$; 
or ({\em ii}) $\{\!\!\{\vec{X} \mid \vec{Y}. p(\vec{W})\}\!\!\}$ where $\vec{X}$ and $\vec{Y}$ are vectors of variables, $\vec{W}$ is vector of parameters and constants such that each variable in $\vec{X}$ or $\vec{Y}$ also occurs in $\vec{W}$.

\end{itemize}  
 %
The semantics of logic programs with such atoms has been defined  (e.g., \cite{faberLP04,nie99b,PelovDB04,SonP05}. Standard syntax for these types of atoms has been proposed and adopted in most state-of-the-art ASP-solvers such as 
\textsc{clasp}~\cite{GebserKNS07} and \textsc{dlv}~\cite{dlv97}. 

\subsection{Action Languages}
\emph{Action languages} \cite{gl98,causal2} are logic-based languages
used to describe actions and their direct and indirect effects. In the rest
of this section, we will concentrate on the action language $\cal B$.

The language of $\cal B$ is based on a signature ${\cal K} = ({\cal L}, {\cal A})$, where
$\cal L$ is a set of ground atoms, referred to as \emph{fluents},
 and $\cal A$ is a set of \emph{action names}. A fluent literal is a literal
 built on $\cal L$.
 A domain description of $\cal L$ is composed of a collection of \emph{axioms} of 
 the following types:
\begin{itemize} 
 \item \emph{Executability Axioms:} these describe the conditions that enable the
         execution of an action, and they have the form $a \executable \ell_1\wedge \dots\wedge \ell_n$,
         where $a\in {\cal A}$ and $\ell_1,\dots,\ell_n$ are fluent literals.
  \item \emph{Dynamic Causal Laws:} these descriptions the direct effects of the execution
          of an action, and they have the form
          $a \causesif{\ell} \ell_1\wedge \dots \wedge \ell_n$
          where $a\in {\cal A}$ and $\ell,\ell_1,\dots,\ell_n$ are fluent literals.
  \item \emph{Static Causal Laws:} these description static dependencies among fluents, and they
          have the form
          $\ell \ifthen \ell_1\wedge\dots\wedge\ell_n$
          where $\ell, \ell_1,\dots,\ell_n$ are fluent literals.
 \end{itemize} 
 A planning problem in $\cal K$ is a tuple $(D, I, O)$, where $D$ is a domain description
 of $\cal K$ and $I, O$ are two consistent sets of fluent literals---denoting, respectively,
 the initial state and the goals of the planning problem.

The semantics of $\cal B$ is based on the definition of transition functions. A set of fluent
literals $S$ 
is consistent and complete if for each $f \in {\cal L}$ we 
have that $\{f,\neg f\}\cap S \neq \emptyset$ and $\{f,\neg f\} \not\subseteq S$. A
set of literals $S$ is closed if, for each static causal law $\ell \ifthen \ell_1\wedge\dots\wedge \ell_n$,
if $\{\ell_1,\dots,\ell_n\}\subseteq S$ then $\ell\in S$. The closure $Cn(S)$ of $S$ is the 
smallest set of fluent literals that contains $S$ and is closed. The states are the complete
and closed set of fluent literals of $\cal L$. 

A domain description defines a transition system $(S,a,S')$, where $a\in {\cal A}$, $S,S'$ are
states, and $S' = Cn((S \cap S') \cup X)$, where $X$ is the set of literals $\ell$ such that
there is a dynamic causal law $a \causesif{\ell} \ell_1\wedge \dots \wedge \ell_n$ and
$\ell_1,\dots,\ell_n \in S$. A trajectory for a planning problem $(D,I,O)$ is a 
sequence $S_0 a_0 S_1 a_1 \dots S_n$ such that $(S_i,a_i,S_{i+1})$ is a transition for
$0\leq i <n$, $I \subseteq S_0$, and $O \subseteq S_n$. A sequence of actions $a_0 \dots a_{n-1}$ is
a plan if there is a trajectory $S_0 a_0 \dots a_{n-1}S_n$.
\fi
%


\section{Beliefs and Truthfulness of Statements} 

In this section, we propose a general framework for representing, and reasoning about, the beliefs held by an agent. The framework also establishes a link between such beliefs and whether the agent may accept as truthful statements made by a third party. The
framework  can be instantiated using specific paradigms for reasoning about actions and change and for non-monotonic reasoning. We build upon the following foundations:
%
 \begin{itemize}
%
\item An \emph{action signature} $\langle F,B,A \rangle$ is given, where $F$ are symbols for fluents (properties of the world whose truth value may change over time),
        $B$ are symbols for an agent's beliefs and $A$ are symbols for actions. The components of the signatures are pairwise disjoint.\footnote{This requirement is imposed to simplify some aspects of the presentation, but is not essential.} The notions of fluent literals and of set $\Sigma$ of states of the domain are introduced as usual.
\item It is possible to  observe the properties of the world and the occurrences of the actions    over time
        (e.g., we observe that John buys a car, John is a student, etc.). 
        Let $H^n$ be the set of all observations up to step $n$ and all recorded action occurrences up to step $n-1$.
 
\item We have adequate knowledge about actions  and their effects (e.g., the action of buying a car requires that the agent has money and its execution will result in the agent owning a car). This knowledge is represented by an action theory $Act$ in a suitable logic that allows reasoning about actions' effects and consequent changes to the world. 
        Let $\Phi_{Act}: \Sigma \times A \rightarrow 2^\Sigma$ denote a suitable transition function describing the evolution of the dynamic system of interest. $\hat\Phi_{Act}$ is the usual extension of $\Phi_{Act}$ to action sequences with $\hat\Phi_{Act}(\sigma, []) = \sigma$ and 
$\hat\Phi_{Act}(\sigma, [a; \beta]) = \bigcup_{\sigma' \in \Phi_{Act}(a, \sigma)} \hat\Phi_{Act}(\sigma', \beta)$ where $\sigma \in \Sigma$ and $\beta$ is an action sequence  and $a$ is executable in $\sigma$.                 
%        denotes the natural extension of $\Phi_{Act}$ to sequence. %A resulting sequence $<\sigma_0,a_0,\sigma_1,a_1,\ldots,\sigma_k>$ is called a \emph{possible path}. 
        %Let us denote with $\models_A$ the entailment relation defined within the logical framework $A$ used to describe $Act$.    

\item We have commonsense knowledge about how an agent of interest forms its beliefs (e.g., an agent may believe that a person attending an expensive school normally comes from a rich family, a person obtaining need-based scholarship s  usually comes from a poor family).
This knowledge is represented by a default theory with preferences $\textit{Def}$,  that enables reasoning about the state of the agent's beliefs. Let us denote with $\models_D$ the entailment relation defined over the default theory framework defining $\textit{Def}$.  
 
\end{itemize} 

The set of observations $O_w\subseteq H^n$ in Example~\ref{ex1} includes the observations such as 
%`John comes from a poor family' at time point $t_0$, 
\begin{itemize}
    \item `John attends an expensive college' at time point $t_1$, and 
    \item `John receives a need-based scholarship' at time point $t_2$.
\end{itemize}
 In this particular example we do not have any  action occurrences.\footnote{Some frameworks for reasoning about actions and change may require the introduction of a NOP action for modeling the example.} Our default theory $Def$ consists of defaults $d_1$ and $d_2$ stated earlier, which allow us to draw conclusions regarding our agent's beliefs. 

\iffalse
%TEMP
\begin{figwindow}[1,r,%
        {\includegraphics[width=0.48\textwidth]{newstep.pdf}},%
        {Answering Question: $(H^n,Act,Def) \models p[t]$?\label{fig:schema} }]
\end{figwindow}
\fi
Let us consider a theory $T = (H^n, Act, \textit{Def})$ and the associated $\Phi_{Act}$ and $\models_D$. First, we are interested in defining the beliefs held by the agent over time. And, then, we are interested in using this information for answering the question of whether a statement $b[t]$,  where $b \in B$ and $t$ is the time step at which the statement was made, can be accepted as truthful. For this, we define the entailment relation $\models$ between $T$ and $b[t]$. Intuitively, the process entails the steps below: %(Figure~\ref{fig:schema}):

\begin{itemize}
\item Compute  $\hat\Phi_{Act}(H^n,\Sigma_0)$, the set of states that are reachable from the initial state given the history $H^n$,
where $\Sigma_0$ denotes the initial state specified by $Act$ and $H^n$, which can be incomplete. 
For compactness, below we abbreviate $\hat\Phi_{Act}(H^n,\Sigma_0)$ with $W[n]$;  

%Compute the set $W[t]$ of fluent literals that are expected to hold at time step \(t\) according to the history;

\iffalse
\memo{
Compute  $\hat\Phi_{Act}(H^n,\Sigma_0)$, the set of states that are reachable from the initial state given the history $H^n$,
where $\Sigma_0$ denotes the initial state specified by $Act$ and $H^n$. 
For simplicity of the presentation, we denote $\hat\Phi_{Act}(H^n,\Sigma_0)$ with $W[t]$. 
}
\fi

\item Determine  the agent's beliefs by finding which propositions are derivable from $\textit{Def}$ and $W[t]$ (using $\models_D$). We note that $W[t]$ can be a set of states and the different notions of entailment such as \emph{skeptical} and \emph{credulous} entailment could be applied here; 

\item Determine if the statement $b[t]$ is consistent with respect to the agent's beliefs. 
\end{itemize}  
%
%The definition of ``expected to hold'' depends on the choice of logic and approach. 
%For the purpose of our discussion, $W[t]$ denotes the set $\hat\Phi_{Act}(H^n)$ of states that are reachable from the initial state given the history $H^n$.
%, where $\Sigma_0$ denotes the initial state specified by $Act$ and $H^n$. 
%For simplicity of the presentation, we denote $\hat\Phi_{Act}(H^n,\Sigma_0)$ with $W[t]$. 
%we assume that $W[t] = \{ l \,|\, l\in \bigcap \hat\Phi_{Act}(H^t) \}$ characterizes
%the fluent literals that are expected to hold at time step $t$ according to history $H^n$ given $Act$. 
%
The entailment relation between $T$ and $b[t]$ can thus be defined by:
%
%T \models p[t] \Leftrightarrow
%\begin{array}{c} 
%%T \models p[t] \\
%% \textit{if and only if} \\
%  \forall W[t]. \left(\begin{array}{c}
%   W[t] = \{z \mid Act \cup O_a \cup O_w \models_A z[t]\} \Rightarrow \\
%                                                                 Def \cup W[t] \models_D p
%                                                                \end{array}\right)
%\end{array} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align} 
T \models b[t] \,\,\Leftrightarrow\,\, \langle W[t], Def \rangle \models_D b \label{entail} 
\end{align} 
 
Note that this definition also allows one to identify elements of $H^t$ that, when obtained, will result in the 
confirmation or denial of $T \models b[t]$. As such, a system that obeys \eqref{entail} can also be used by users who are 
interested in  what they need to do in order to believe in a statement about $b$ at the time step $t$,
 given their beliefs about the behavior of the observed agents. 


%
%\smartdiagram[flow diagram:horizontal]{Edit,
%  \LaTeX, Bib\TeX/ biber, make\-index, \LaTeX}
\iffalse\begin{figure}
\centering
\resizebox{1.6in}{!}{% 
\smartdiagram[priority descriptive diagram]{
  $W[t]$ model of $A \cup O_a \cup O_w$ restricted to time step $t$,
  $\forall W[t]. (D \cup W[t] \models_D p)$?}
}  
\caption{Answering $(O_a,O_w,A,D) \models p[t]$?}
\label{fig:schema} 
\end{figure} \fi
%
To develop a concrete system for reasoning about truthfulness of agents using \eqref{entail}, specific formalizations of $Act$ and $\textit{Def}$ need to be developed. There is a large body of research related to these two areas, and deciding which one to use depends on the system developer. 
Well-known formalisms for reasoning about actions and change, such as action languages \cite{GelfondL98}, situation calculus \cite{Reiter01}, etc., can be employed for $Act$ and $\Phi_{Act}$.  
Approaches to default reasoning with preferences, such as those proposed in  \cite{brew99,BrewkaE00,DelgrandeST03,GelfondS98}), can be used for $\textit{Def}$ (and $\models_D$). In addition, 
let us note that, in the literature, $\models_D$ can represent \emph{skeptical} or \emph{credulous} reasoners; and  the model does not specify how observations are collected. Deciding which type of reasoning is suitable or how to collect observations is an important issue, but it is application-dependent and beyond  the scope of this paper. 
%In this paper, we use observations  in a broad sense: it might be something that we learn some facts about a property of the world, or we actually observed it (via the execution of an action), etc. 
%



\section{Reasoning about Beliefs and Truthfulness Using Answer Set Programming}  

We will now present a concrete system for reasoning about truthfulness of agents using \emph{Answer Set Programming (ASP)} \cite{MarekT99,GelfondL91}.  We select ASP as the host language since there exist several approaches to reasoning about actions and changes as well as default reasoning using ASP. This enables  the seamless integration of the two steps involved in ~\eqref{entail} into a single system. 
Several efficient ASP solvers are available, that enable the use of the proposed system in practical applications. In particular, we will use answer sets (and projections of answer sets over specific time points) to capture $W[t]$, and use established encodings of action theories and defaults in ASP.
  
 
\subsection{Background: Answer Set Programming}

A logic program $\Pi$ is a set of rules of the form
%
\begin{equation}
\label{lprule1}
 a_0 \leftarrow a_1,\ldots,a_m,\naf a_{m+1},\ldots,\naf a_n
\end{equation}
%
or
%
\begin{equation}
\label{lprule2}
 \bot \leftarrow a_1,\ldots,a_m,\naf a_{m+1},\ldots,\naf a_n
\end{equation}
%
where $0 \le m \le n$, each $a_i$ is an atom of a first-order
language $\cal P$, $\bot$ is a special symbol denoting the
truth value \emph{false}, and $\naf$ is a connective called {\em
negation-as-failure}. A negation as failure literal (or
naf-literal) is of the form $\naf a$ where $a$ is an atom. For a
rule of the form (\ref{lprule1})-(\ref{lprule2}), the left and
right hand side of the rule are called the \emph{head} and the
\emph{body}, respectively. A rule of the form (\ref{lprule2}) is
also called a constraint.

Given a logic program $\Pi$, we will assume that each rule in
$\Pi$ is replaced by the set of its ground instances so that all
atoms in $\Pi$ are ground. Consider a set of ground atoms $X$. The
body of a rule of the form (\ref{lprule1}) or (\ref{lprule2}) is
satisfied by $X$ if $\{a_{m+1},\ldots,a_n\} \cap X = \emptyset$
and $\{a_{1},\ldots,a_m\} \subseteq X$. A rule of the form
(\ref{lprule1}) is satisfied by $X$ if either its body is not
satisfied by $X$ or $a_0 \in X$. A rule of the form
(\ref{lprule2}) is satisfied by $X$ if its body is not satisfied
by $X$. An atom $a$ is supported by $X$ if $a$ is the head of some
rule of the form (\ref{lprule1}) whose body is satisfied by $X$.

For a set of ground atoms $S$ and a program $\Pi$, the reduct
of $\Pi$ with respect to $S$, denoted by $\Pi^S$, is the program
obtained from the set of all ground instances of $\Pi$ by deleting

\begin{enumerate}
\item each rule that has a naf-literal $\naf a$ in its body with
$a \in S$, and

\item all naf-literals in the bodies of the remaining clauses.
\end{enumerate}

$S$ is an \emph{answer set (or a stable model)} of $\Pi$ if it
satisfies the following conditions.

\begin{enumerate}
\item  If $\Pi$ does not contain any naf-literal
(i.e. $m=n$ in every rule of $\Pi$) then $S$ is the smallest set
of atoms that satisfies all the rules in $\Pi$.

\item If the program $\Pi$ does contain some naf-literal
($m < n$ in some rule of $\Pi$), then $S$ is an answer set of
$\Pi$ if $S$ is the answer set of $\Pi^S$. (Note that $\Pi^S$ does
not contain naf-literals, its answer set is defined in the first
item.)
\end{enumerate}

A program $\Pi$ is said to be \emph{consistent} if it has an
answer set. Otherwise, it is inconsistent.

\iffalse 

\textcolor{red}{We should have at least a better section on ASP – to make things self-contained, mention skeptical and credulous reasoning in ASP – what does it mean – this will be helpful for the definitions later. We may also want to talk about models of a history from \cite{BalducciniG03}.}


ASP is a programming paradigm \cite{Niemela99,MarekT99}  based on logic programming under the answer set semantics \cite{GelfondL90}. 
A logic program $\Pi$ is a set of rules of the form 
 $$c  \leftarrow a_1,\ldots,a_m,\naf a_{m+1},\ldots,\naf a_n$$ 
%
where $0 \le m \le n$,   $a_i$'s and $c$ 
are  ground literals in a first order language,\footnote{Rules with variables represent the set of all 
        of their  ground instances.}   
and $\naf$ represents {\em default negation}. 
A rule is satisfied by a set of literals $S$ if $c \in S$, or $\{a_1,\ldots,a_m\} \setminus S \ne \emptyset$, or $\{a_{m+1},\ldots,a_{n}\} \cap S \ne \emptyset$. A consistent set of literals $S$ is a model of a program if it satisfies all the rules of the program. $S$ is an answer set of a program if it is a consistent minimal model of the program obtained by {\em (i)} deleting all rules with some $a \in S$ such that $not \: a$ appears in the rule; and {\em (ii)} deleting all  $not \: a$ from the remaining rules.  Programs without an answer set are said to be inconsistent. 

\fi 


Several extensions (e.g.,  \emph{choice atoms}, \emph{aggregates}, etc.) have been introduced to simplify the use of ASP. We will use and explain them whenever it is needed. Answer sets can be computed using answer set solvers. Extended syntactic notations have been proposed to facilitate the encoding of commonly used patterns (e.g., aggregates). In this paper, we use the  syntax of ASP used in  {\sc Clasp}\footnote{
  \url{https://github.com/potassco}
} or DLV\footnote{\url{http://www.dlvsystem.com/}
}. 




%
%A negation-as-failure literal (or
%naf-literal) is of the form $\naf a$ where $a$ is a literal. For a
%rule of the form (\ref{lprule1}), the left and
%right hand sides of the rule are called the \emph{head} and the
%\emph{body}, respectively. Both the head and the body can be empty. 
%When the head is empty, the rule is called a {\em constraint}. 
%When the body is empty, the rule is called a {\em fact}. 
%


\subsection{Reasoning with No Observations about Action Occurrences}

Let us start with a simpler case---where we do not worry about action
occurrences. We represents the components of the theory by means of keywords 
$stm$ (for $statement$), $obs$ (for $observed$), $rule$, $default$, and $prefer$. 
Fluent literals are defined as usual. Similarly, symbols for beliefs yield the notion of belief literal. By literals we mean fluent literals or belief literals. $\overline{l}$ 
denotes the complement of literal $l$. Agents can make statements about belief literals.  
Statements are given in the form {$stm(b,s)$}
where  $s$ is a non-negative integer (representing a time step) and $b$ is a belief literal.  
For example, the statement `John said that he is poor'  can be stated as 
$stm(poor, 0)$. 

%
%\begin{definition} 
%A \emph{statement} about a   literal $p$ is of the form 
%\begin{align}\label{stat}
%stm(p,  s)  
%\end{align} 
%where  $s$ is a non-negative integer. 
%\end{definition} 
%For example, John said that he is poor. This can be stated as 
%$
%stm(poor, 0) 
%$
%says that $poor$ is true at time step 0. 

%Another example is that John said that he is poor. This can be stated as 
%\[
%stm(poor, 0) 
%\]
%states that $poor$ is true at time step 0. 

\iffalse
We can make observations about fluent literals. 
\begin{definition} 
An observation about a fluent literal $P$ is of the form 
\begin{equation}\label{obs}
obs((P,   S)  
\end{equation} 
where  $S$ is a non-negative integer. 
\end{definition} 
Note that $obs$ is understood in a broad sense: it might be that we learn some facts about the fluent literal, or we actually observed it (via the execution of a sensing action), etc. 

For example, if we observe that John is not sick, we use 
\[
obs(\neg sick, 0). 
\]


Our reasoning module consists of a collection of rules and defaults, possibly with priorities among defaults. 
\begin{itemize} 
\item A \emph{rule} is of the form 
\begin{align}\label{rule}
rule(r, head, body)
\end{align} 
where $r$ is the name of the rule, $head$ is a fluent literal, and $body$ is a collection of literals. A rule \eqref{rule} says that whenever $body$ holds then $head$ must also hold. 
\item A \emph{default} is of the form 
\begin{align}\label{default}
default(d, head, body)
\end{align} 
where $d$ is the name of the default, $head$ (or \emph{conclusion}) is a fluent literal, and $body$ is a collection of literals.  A default \eqref{default} says that, normally, if $body$ holds then $head$ also holds. 
\item A \emph{preference} between two defaults is expressed by 
\begin{align}\label{pref}
prefer(d_1, d_2, body)
\end{align} 
where $body$ is a set of fluent literals and $d_1$ and $d_2$ are names of two defaults. This says that 
the default $d_1$ is  preferred to $d_2$ whenever $body$ holds.

\end{itemize} 

\fi

\begin{definition} 
A \emph{knowledge base (KB)} over an action signature  $\langle F,B,A \rangle$, 
is a tuple $\langle  O, RD \rangle$ where: 
\begin{itemize} 
\item $O$ is a set of observations, each of the form 
%
\begin{align}\label{obs}
obs(l,  s)  
\end{align} 

 
where $l$ is a fluent literal and $s$ is a non-negative integer; 
\item $RD$ is a collection of expressions of the form 
%
\begin{align}
rule(r, b, body)\label{rule} \\
default(d, b, body) \label{default} \\
prefer(d, d', body) \label{pref}
\end{align} 

where $r$, $d$ and $d'$ are constants that do not occur in the signature, 
$b$ is a belief literal and $body$ is a set of literals.  
\end{itemize}  
\end{definition} 
Statements \eqref{rule}, \eqref{default}, and \eqref{pref} are referred to as a 
 \emph{rule}, a \emph{default}, and a 
 \emph{preference} between defaults, respectively. 

Intuitively, $r$ and $d$ are used to name rules and defaults. 
%
%We will next define the notion of consistent knowledge bases. 
We say that a set of observations $O$ is \emph{consistent} if, for each time step $s$ and fluent literal $l$, $\{obs(l,s), obs(\neg l, s)\} \not\subseteq O$. 
Following the conventional approach from the literature, we make the assumptions that the preference relation in $RD$ is an 
\emph{acyclic preference} and  that it is a transitive and irreflexive relation, i.e., there exists no sequence of preference statements $prefer(d_i, d_{i+1}, body_i)$, $i=1,\ldots,k$, in $RD$ such that $d_1 = d_{k+1}$ and $\bigcup_{i=1}^k body_i$  is consistent. 

\iffalse 
I added 
Following the conventional assumption in the literature, we will make the following assumptions about the preference relation in $RD$: 
\begin{itemize}
    \item \emph{Non-Cyclic Preference}: we assume that the preference relation between defaults represents a transitive and irreflexive relation, i.e., there exists no sequence of preference statements $prefer(d_i, d_{i+1}, body_i)$, $i=1,\ldots,k$, in $RD$ such that $\bigcup_{i=1}^k body_i$  is consistent. 
    \item \emph{Belief in Truth}: we assume that for each belief literal $l$, $RD$ contains the default $default(d_b, l, [l])$ and for each default $default(d, \overline{l}, body)$, if $body \ne [\overline{l}]$ is not empty then $prefer(d, d_b, [])$ belongs to $RD$. This default states that if $b$ is true then it is believed by default.
\end{itemize}
\fi 

A set of rules $RD$ is \emph{consistent} if, for any consistent set of fluent literals $X$, the logic program 
$\{b \leftarrow body  \mid rule(r, b, body) \in RD\} \cup \{l \leftarrow \:\mid\: l\in X\}$
is consistent.
%
\begin{definition}
A $KB = \langle O, RD \rangle$ is \emph{consistent} if $O$ and $RD$ are consistent. 
\end{definition} 
%
Our goal is to identify the agent's beliefs and to use them to determine whether  a statement $stm(p, s)$ is true at a time step $t$ given $KB$. We will develop a program $\Pi(KB)$ to answer this question. We assume a finite horizon of steps, denoted by $s(0), \ldots, s(k)$. We use $h(L,S)$ to encode that fluent literal $L$ is true at step $S$. We use $believes(B,S)$ to state that the agent believes $B$ at step $S$. Furthermore, to simplify the use of $\Pi(KB)$ with current ASP solvers, we  encode a rule of the form \eqref{rule}  using {\bf (a)} the ASP atom $rule(r, head, name)$ and
{\bf (b)} the set of ASP atoms $\{mbr(l, name) \mid l \in body_{r}\}$, where \(name\) is a unique identifier for the rule and $mbr$ stands for `member'; similar encodings are used for  \eqref{default} and \eqref{pref}. 
This method of encoding is similar to what is used  in answer set planning and allows for the separation between  the encoding of the problem and the rules for reasoning with it.
The main challenge to be addressed lies in the interaction between observations and defaults. 
We apply the following principles:
%
\begin{itemize}
\item[]{\bf (O)}ptimistic: The most recent observation reflects the true state of the world;
\item[]{\bf (SK)}eptical: When conflicting conclusions can be drawn then do not believe in any.
\end{itemize}  

\iffalse
An interesting challenge is related to the encoding of $\Phi_{Act}$ for the case in which the initial state is not completely specified.
One possible option is to introduce an explicit encoding of the \emph{awareness axiom} \cite{GelfondKahl14a}, stating that the agent is aware that fluent literals may be either true or false:
\[
h(F,S) \vee h(L,S) \leftarrow fluent(F),neg(F,L).
\]
This approach allows to map each trajectory stemming from a different possible initial state to an answer set of the program. %Set  $W[t]$ is then obtained by computing the cautious consequences of the ASP program.
\fi

%In this paper, we discuss a slightly different, more economical approach, under the assumption that actions are deterministic. First of all,
%
The first part of $\Pi(KB)$ contains the set of facts encoding $O$ and $RD$ as described above. Their semantics is captured by the following rules.\footnote{In the following, The  predicate $neg$ encodes the complement of a literal, i.e, $neg(f, \neg f)$ and $neg(\neg f, f)$ are facts of the program.}
%
%The first set of rules encodes the \emph{awareness axiom} \cite{GelfondKahl14a}, stating that the agent is aware that fluent literals may be either true or false.
%\[
%h(F,S) \vee h(L,S) \leftarrow fluent(F),neg(F,L).
%\]

The first set of rules is used to  reason about observations of the form 
\eqref{obs}:
%
\begin{eqnarray}  
h(L, S)  & \leftarrow &   obs(L,S_1), s(S),  S_1 \le S, \label{lp:obs} \\
&&\naf nr(L, S_1, S).    \nonumber\\
%               \naf \:not\_most\_recent(P, S_1, S).  \nonumber \\
% not\_most\_recent(P, S_1, S) \leftarrow s(S_1),  \label{lp:obs1}  \\ 
  nr(L, U, S)  & \leftarrow &  s(U), neg(L,L_1), obs(L_1, V), s(S), \label{lp:obs1} \\
  &&U {<} V, V {<} S.  \nonumber  %\\ 
%obs(\overline{P}, S_2), s(S),  S_1 < S_2, S_2 < S. \nonumber
\end{eqnarray} 

These rules encode the principle {\bf (O)}. Rule \eqref{lp:obs1} states that $nr(L,S_1)$ is true if there exists a more recent observation of  $\neg L$. Rule \eqref{lp:obs} indicates that an observation stays true if every conflicting observation is `older'.
 
 For reasoning about statements of the form \eqref{rule}, we have\footnote{
 We would like to thank the anonymous reviewer for the suggestion on how to encode this rule for better performance.  
%%believes(H,S) :- s(S), rule(D,H,M),
%%#sum{1,L : mbr(L,M);
%%-1,L : mbr(L,M), h(L,T);
%%-1,L : mbr(L,M), believes(L,T)} == 0. 
 }:  %
%
\begin{equation}\label{lp:rule}
\begin{array}{ll}
believes(H, S) \leftarrow & s(S),  rule(D, H, M),  \\
        & N  == \# count \{L : mbr(L, M)\},\\
        & \# count \{L : mbr(L, M),\\
        &\hspace*{.8in}h(L, T) \}== N_h,  \\ 
        & \# count \{L : mbr(L, M),\\
        &\hspace*{.44in}believes(L, T) \}== N_b, \\
        & N_h+N_b==N.  \\ 
%
%& N = \# count \{L : mbr(L, M),  h(L, S) \},\\ 
%     & N  = \# count \{L : mbr(L, M)\}.
      \end{array}
\end{equation} 

%
Intuitively, (\ref{lp:rule}) states that, if the body of a $rule$ statement is satisfied, then its head must be true. Recall that, according to the syntax given earlier, the head is a belief -- hence the expression $believes(H,S)$ in (\ref{lp:rule}). The `$==$' symbol denotes identity at the level of ground terms.\footnote{The variables involved are replaced by a constant during the grounding process and the identity is checked during the solving phase.} Checking whether the body is satisfied is compactly achieved by leveraging the $\#count$ aggregate of the ASP Core 2 language\footnote{Details can be found in \cite{ASPCore2Standard}}, which calculates the cardinality of a set. For instance, the expression $\# count \{L : mbr(L, M), h(L, T) \}$ finds number of literals that are in the body of the rule ($mbr(L,M)$) and hold at time step $T$ ($h(L,T)$). The combination of aggregates present in (\ref{lp:rule}) checks that the number of literals that hold or are believed equals the size of the body of the rule.\footnote{Recall that fluent symbols and belief symbols are disjoint. }

 For reasoning about defaults  of the form \eqref{default} we have:
% 
%%\iffalse
%{\small
\begin{eqnarray}
app(D, H, S) & \leftarrow  &  s(S), default(D, H, M), \nonumber \\
        & & N  == \# count \{L : mbr(L, M)\},    \nonumber\\
        & & \# count \{L : mbr(L, M),  \nonumber\\
        & & \hspace*{.8in}h(L, T) \}== N_h,  \nonumber\\ 
        & & \# count \{L : mbr(L, M), \nonumber\\
        & & N_h+N_b==N. \label{lp:app} \\ 
%     & & NC  = \# count \{L : mbr(L, M)\}, \nonumber \\ 
%    & & \# count \{L : mbr(L, M),     h(L, T) \} == NC. \nonumber \\ 
ab(D, H, S)& \leftarrow &   s(S), app(D, H, S), \nonumber\\
    & &  neg(H, H_1), app(D_1, H_1, S),  \nonumber \\
    &&   \naf defeated(D_1, H_1, S).  \label{lp:ab}      \\
defeated(D, H, S) & \leftarrow  & s(S), app(D, H, S), \nonumber\\
        & &  neg(H,H_1), \nonumber \\
        & &  h(H_1, S).  \:\:\:  \label{lp:defeat01} \\ 
defeated(D, H, S) & \leftarrow  & s(S), app(D, H, S), \nonumber\\
        & &  neg(H,H_1), \nonumber \\
        & &  believes(H_1, S).  \:\:\:  \label{lp:defeat0} \\ 
defeated(D, H, S) & \leftarrow  & s(S),  app(D, H, S),    \nonumber \\ 
        & &  neg(H,H_1), \nonumber \\ %
        & & app(D_1, H_1, S), \nonumber\\
        & & prefer(D_1, D, S).   \label{lp:defeat}\\
believes(H, S) & \leftarrow & app(D, H, S), \nonumber \\
        & & \naf ab(D, H, S), \nonumber\\
        & & \naf defeated(D, H, S).  \label{lp:holds}
%    
%defeated(D, H, S)& \leftarrow &  s(S), app(D, H, S), h(\overline{H}, S).\label{lp:defeat0} \\
%    &&  app(D, H, S),  \nonumber \\
%    &&  h(\overline{H}, S). \nonumber \\       
%defeated(D, H, S)& \leftarrow  & s(S),  app(D, H, S), \label{lp:defeat} \\
%    && app(D_1, \overline{H}, S), prefer(D_1, D, S).  \nonumber \\
%    &&  prefer(D_1, D, S).  \nonumber      \\
%h(l_1, S), \ldots, h(l_k, S), \naf ab(d, l, S)   \label{lp:def}\\
%b(l, S) \leftarrow b(l_1, S), \ldots, b(l_k, S), \naf ab(d, l, S)   \label{lp:def_bel}\\
%app(d, l, S) \leftarrow h(l_1, S), \ldots, h(l_k, S) 
%h(H, S)& \leftarrow  app(D, H, S), \naf ab(D, H, S),  \label{lp:holds} \\
% \naf ab(D, H, S), \nonumber \\ 
% & \naf defeated(D, H, S). \nonumber 
\end{eqnarray} 
%} 
%%\fi
%
\iffalse
{
        \[
        \begin{array}{llr}
     app(D, H, S)  \leftarrow &  s(S), default(D, H, M),  \nonumber\\
        & N  = \# count \{L : mbr(L, M)\},  & \label{lp:app} \\
        & \# count \{L : mbr(L, M),\\
        &\hspace*{.8in}h(L, T) \}= N_h,  \\ 
        & \# count \{L : mbr(L, M),\\
        &\hspace*{.44in}believes(L, T) \}= N_b, \\
        & N_h+N_b=N.  \\ 
        ab(D, H, S) \leftarrow  & s(S), app(D, H, S), neg(H,H_1), & (10)\\   %\label{lp:ab}
        &  app(D_1, H_1, S), \\
        & \naf defeated(D_1, H_1, S).        \\
        defeated(D, H, S) \leftarrow  & s(S), app(D, H, S), neg(H,H_1), \\
        & believes(H_1, S).  \:\:\: & (11) \\ %\label{lp:defeat0} 
        defeated(D, H, S) \leftarrow  & s(S),  app(D, H, S), neg(H,H_1), & (12) \\  %\label{lp:defeat}
        & app(D_1, H_1, S), \\
        & prefer(D_1, D, S).   \\
        believes(H, S) \leftarrow & app(D, H, S), \naf ab(D, H, S),\\
        & \naf defeated(D, H, S).  & (13) \\ %\label{lp:holds}
        \end{array}
        \]
        }
%
\fi

 
The rule for {\it app} defines when a default is \emph{applicable}, i.e., when its body is satisfied. The interaction between defaults and rules in the knowledge base are dealt with using rules for {\it ab} and {\it defeated}. The first rule
        for {\it defeated}  dismisses the applicability of a default if the complement of its conclusion is already established. The second rule for {\it defeated} expresses that a default is defeated if there is a  preferred default with a conflicting conclusion that is applicable.
The rule for {\it ab} enforces the principle {\bf (SK)}. It states that a default $d$ should be blocked if there is another default with the conflicting conclusion ($h'$ in $neg(h, h')$), which is not defeated.
Finally, the last rule  enforces the application of any default that is applicable and not otherwise blocked.
 
 \setcounter{equation}{13}
 
 For reasoning about preferences of the form \eqref{pref}, we employ the rules:
% 
{
\begin{equation}\label{lp:pref}
\begin{array}{ll}
prefer(D_1, D_2, S) \leftarrow &  s(S),   prefer(D_1, D_2, M),   \\
     & N  = =\# count \{L : \\
     &  \hspace*{.3in} mbr(L, M)\},  \\ 
     & \# count \{L : mbr(L, M),  \\
     &  \hspace*{.3in}  h(L, S) \} {=} =N_h, \\
     & \# count \{L : mbr(L, M), \\
     & \hspace*{.3in} believes(L, S) \} {=} =N_b, \\
     & N_h+N_b==N.
      \end{array}
\end{equation} 
}
%
This rule defines when a preference among two defaults can be applied. 
%
%\iffalse 
%Rule~\eqref{lp:app} defines when a default is \emph{applicable}, i.e., when its body is satisfied. The interaction between defaults and rules in the knowledge base are dealt with using rules~\eqref{lp:ab}---\eqref{lp:defeat}. Rule~\eqref{lp:ab} enforces the principle {\bf (SK)}. It says $ab(d, h, s)$ that if there is another default with the conflicting conclusion ($\overline{h}$) 
%which is not defeated then $ab(d, h, s)$ is true. This atom will be used to suppress the conclusion of $d$.   
%\eqref{lp:defeat0} dismisses the applicability of a default if the contrary of its conclusion is already established. On the other hand, rule \eqref{lp:defeat} expresses that a default is defeated if there is a more preferred default with conflicting conclusion that is applicable.  
%
%
% 
%
%\item Rules for reasoning about beliefs  
%\begin{align}
%b(L, S) \leftarrow h(L, S)  \label{lp:belief}   
%\end{align} 
%
%
%\item For interacting between defaults, we have the following rules: 
%
%%\memo{
%%}
%
%\begin{center} 
%  \begin{tikzpicture} 
%%    \tikzstyle{every node}=[draw,shape=circle} 
%%    \node (s1) at (0:0) {$\alert{s_3:}$}   
%%      \draw[fill=blue!20,style=dashed] (-1.0,-1.0) rectangle (3.0,3.0)  ;
%
%      \draw (0,0) circle(0.4) node {$\textcolor{red}{\small s{-}1}$} ;
%      \draw  (2,0) circle(0.4) node  {$\textcolor{red}{\small s}$} ;
%     \draw  (4,0) circle(0.4) node {$\textcolor{red}{\small s{+}1}$} ;  
% 
%     \draw[-,style=dashed] (-1.6,0) -- (-0.4,0 )  ; 
%     \draw[-] (0.4,0) -- (1.6,0)  ;  
%     \draw[-] (2.4,0) -- (3.6,0 )  ; 
%     \draw[-,style=dashed] (4.4,0) -- (5.6,0 )  ; 
% 
%  \end{tikzpicture} 
%\end{center} 
%
%
%\begin{align}\label{lp:ab} 
%ab(D_1, L, S) \leftarrow \: \: &     default(D_1, L, Body_1),   \\
%    &      app(D_1, L, S), \nonumber \\
%    &      default(D_2, \overline{L}, Body_2), \nonumber \\
%    &      app(D_2, \overline{L}, S), \nonumber \\
%    &      \naf app(D_2, S-1).  \nonumber
%\end{align} 
%
%This rule encodes the principle {\bf (P3)}. 
%
%\fi
%
%
%
%% We will assume that things behave in accordance to the law of inertia. So, we will add to the the program the rule 
%%
%% \begin{align}
%% h(l, S+1) \leftarrow h(l, S),  \naf h(\overline{l}, S)   \label{lp:inertial} \\
%% b(l, S+1) \leftarrow b(l, S),  \naf b(\overline{l}, S)   \label{lp:inertial_bel} 
%% \end{align} 
%
%

In summary, for a $KB = \langle O, RD \rangle$, $\Pi(KB)$ consists of rules~\eqref{lp:obs}--\eqref{lp:pref} and the set of facts encoding $RD$ and $O$.

%
\begin{proposition}
For each consistent $KB$,  there exists no answer set of $\Pi(KB)$ that contains $believes(l, t)$ and $believes(\overline{l},t)$ for some 
literal $l$ and time step $t$. 
\end{proposition} 
%
\noindent
{\bf Proof.}
The proof of this proposition relies on the fact that $believes(l, t)$ is in an answer set $A$ if the logic program  rule \eqref{lp:rule} (or \eqref{lp:holds}) is satisfied by $A$. If a rule \eqref{lp:rule} is satisfied by $A$ then a $KB$ contains a rule $rule(r,l,body)$ and $body$ is satisfied by $A$. Because of the consistency of $KB$, we know that there exists no rule $rule(r', \overline{l}, body')$ such that $body'$ is satisfied by $A$. For, otherwise, $KB$ is inconsistent. This will imply that for every default $default(d, \overline{l}, body')$, if the default is applicable, then the body of the rule \eqref{lp:defeat0} is satisfied and therefore $defeated(d)$ belongs to $A$. Consequently, \eqref{lp:holds} is not satisfied for the default $d$, and hence, $believes(\overline{l}, t)$ cannot belong to $A$.

Similar argument can be used to argue for the case that two defaults, one supporting $l$ and another supporting $\overline{l}$, given that $KB$ is consistent can never be applied at the same time to create both $believes({l}, t)$ and $believes(\overline{l}, t)$ in an answer set.
\hfill\qedsymbol{}





%
\begin{example} \label{ex2}  
The story in Example~\ref{ex1} 
can be represented by the $KB_1 = \langle  O_1, RD_1 \rangle$ where 
%$O_1 = \{obs(in\_college, 1), obs(has\_scholarship, 2)\}$ and  
%$RD_1 = \{default(d_1, \neg poor, [in\_college])$,
%$default(d_2, poor, [has\_scholarship])\}$.

{
\[
\begin{array}{lll}
O_1 & = & \left\{
\begin{array}{l} 
%stm(poor, 0). \\
obs(in\_college, 1). \\ 
obs(has\_scholarship, 2).
\end{array}
\right \},   \\
 RD_1 & = & \left\{
\begin{array}{l} 
default(d_1, \neg poor, [in\_college]). \\
default(d_2, poor, [has\_scholarship]).  
\end{array}
\right \} 
\end{array} 
\]
}


The statement in Example~\ref{ex1} can be represented by  $stm(poor, 0).$ 
Consider   $\Pi(KB_1)$ with $k=0,1,2$:  
%
\begin{itemize} 
\item   $k=0$: since there is no observation, no answer set of $\Pi(KB_1)$ contains any atom of the form $believes(poor, 0)$; %where $l$ is a literal.
 
\item   $k=1$: because of $obs(in\_college,1)$, $h(in\_college,1)$ is true, 
%% is in every answer set of $\Pi(KB_1)$.  
$d_1$ is applicable but $d_2$ is not.  So, every answer set of $\Pi(KB_1)$ contains    $h(in\_college, 1)$ and $believes(\neg poor, 1)$.  

\item   $k=2$: %every answer set of $\Pi(KB_1)$ contains 
$h(has\_scholarship,2)$, $h(in\_college,1)$, and $h(in\_college,2)$ are true and $d_1$ is applicable at 1 and 2; $d_2$ is not applicable at   1 but is applicable at   2.  Hence, every answer set of $\Pi(KB_1)$ contains $believes(\neg poor, 1)$.  However, 
neither $believes(poor, 2)$ nor $believes(\neg poor, 2)$ belongs to every answer sets of $\Pi(KB_1)$.  

\end{itemize}  



% Rules for reasoning about statements of the form \eqref{stat} 
% \begin{align}\label{lp:stat}
% b(p, s) \leftarrow stm(p,s), \naf h(\overline{p},s) 
% \end{align} 
% This rule encodes the principle {\bf (P2)}. 


%
%\begin{verbatim}
%s(0..length).
%
%stm(poor, true, 0).
%obs(attend, 1).
%obs(scholarship, 2).
%
%default(d1, -poor, [attend]). 
%default(d2, poor, [scholarship]).
%
%h(poor,T):- s(T), stm(poor,T), not h(-poor,T).  
%
%h(-poor, T):- s(T), h(attend, T), not ab(attend, -poor, T).        
%
%h(F, T) :- s(T), obs(F, T).                                                 
%
%h(poor, T) :- s(T), h(scholarship, T), not ab(attend, poor, T).
%
%ab(attend, -poor, T) :- s(T), h(scholarship, T).
%
%h(F, T+1) :- fl(F), s(T), T<length, h(F, T), not h(-F, T+1). 
%h(-F, T+1) :- fl(F), s(T), T<length, h(-F, T), not h(F, T+1). 
%
%fl(poor).
%fl(attend).
%fl(scholarship).
%\end{verbatim} 
\end{example} 
 
Given a $KB=\langle O, RD \rangle$, 
an integer $k$, and a $stm(b,s)$, we 
are interested in determining the truthfulness of the statement 
at time steps $s \le t \le k$. %This is defined as follows. 

\begin{definition} \label{kb:entail}
Let $KB =  \langle O,RD \rangle$ be a knowledge base. Let $stm(b,s)$ be a statement about a belief literal $b$ at 
the time step $s$. For each time step $t$ such that $t \ge s$, we say that 
\begin{itemize} 
\item $KB \models {+}stm(b,s)@t$ (or $stm(b, s)$ 
is believed to be \emph{true} w.r.t. $KB$ at   $t$)  if  $believes(b,t) \in A$ for every answer set $A$ of $\Pi(KB)$; 
\item $KB \models {-}stm(b,s)@t$ (or 
$stm(b, s)$ 
is believed to be \emph{false} w.r.t. $KB$  at  $t$)  if  $believes(\overline{b},t) \in A$ for every answer set $A$ of $\Pi(KB)$;
\item $KB \not\models {\pm}stm(b,s)@t$ (or 
$stm(b, s)$ 
is \emph{undecided} w.r.t. $KB$  at  $t$)    
if $KB \not\models +stm(b,s)@t$ and $KB \not\models -stm(b,s)@t$.

\end{itemize}  
\end{definition} 

Intuitively, $KB  \models {+}stm(b,s)@t$ (resp. $KB  \models {-}stm(b,s)@t$) says that at the time step $t$, at the same time step or later than the time step that the statement about $b$ is made, the statement is believed to be true (resp. believed to be false). $KB \not\models {\pm}stm(b,s)@t$ states that there is no information that supports or denies the statement at the time step $t$. Observe that $l$, $t$, and $s$ are constants representing the question whether or not $stm(b,s)$ is believed to be true at step $t$. As such, requiring $believes(b,t)$ to be in every answer set of the program 
does not mean that $believes(b,t)$ belongs to every answer set for every $t$ such that $s \le t \le k$. 
Observe that this entailment can be computed in ASP by the following rules: 
%
{
\begin{eqnarray}
\mathtt{t}(H, T) & \leftarrow & stm(H, S), s(T), T \ge S, \nonumber \\
        &  & believes(H, T). \label{q1} \\ 
\mathtt{f}(H, T) & \leftarrow & stm(H, S), s(T), T \ge S, neg(H,H_1),  \nonumber\\ 
& & believes(H_1, T).  \label{q2}\\
\mathtt{u}(H, T) & \leftarrow & stm(H, S), s(T), T \ge S, neg(H,H_1), \nonumber  \\
& & \naf believes(H, T), \nonumber \\
&&  \naf believes(H_1, T). \label{q3}
\end{eqnarray}
}
%
Let $\Pi_Q = \Pi(KB) \cup \{\eqref{q1}{-}\eqref{q3}\}$. It can be shown that 
 $KB  \models {+}stm(b,s)@t$, $KB  \models {-}stm(b,s)@t$, and  $KB  \models {\pm}stm(b,s)@t$ 
 correspond to $\Pi_Q \models \mathtt{t}(b,t)$, $\Pi_Q \models \mathtt{f}(b,t)$, and 
 $\Pi_Q \models \mathtt{u}(b,t)$, respectively. Hence, we can compute the truthfulness of a statement 
using two calls to an ASP-solver. 
 
 %For example, if the program $$\Pi_Q \cup \{\leftarrow \naf \mathtt{f}(l,t), \naf \mathtt{u}(l,t).\}$$ does not have an answer set and the program $$\Pi_Q \cup \{\leftarrow \naf \mathtt{t}(l,t)\}$$ has an answer set then we can conclude that $\Pi_Q \models \mathtt{t}((l, t)$. 

\begin{proposition} 
For $\mathtt{x} \in \{\mathtt{t, b, u}\}$,
if  $\Pi_Q \cup \{\leftarrow  \mathtt{x}(b,t).\}$ does not have an answer set and  
$\Pi_Q \cup \{\leftarrow \naf \mathtt{x}(b,t)\}$   has an answer set then   $\Pi_Q \models \mathtt{x}(b, t)$. 
\end{proposition}  
% 
\noindent
{\bf Proof.}
Observe that if $\Pi_Q \cup \{\leftarrow \mathtt{x}(b,t).\}$ does not have an answer set, this
implies that  either $\Pi_Q$ has no answer set or every answer set of $\Pi_Q$
contains $\mathtt{x}(b,t)$. The second condition implies that $\Pi_Q$ has at least one answer set. 
This implies that  $\Pi_Q \models \mathtt{x}(b, t)$.  
\hfill\qedsymbol{}

\medskip
The above proposition allows us to compute the entailment $KB \models {+}/{-}/{\pm} stm(b,s)@t$ by making two calls to the answer set solver. 
%%\memo{formalize as proposition} 
% 
%It is easy to see that the following holds 
We can show that  
 $KB_1 \not\models {\pm}stm(poor,0)@0$;
 $KB_1 \models   {-}stm(poor,0)@1$; and
 $KB_1 \not\models {\pm}stm(poor,0)@2$. 

%%
%  \begin{itemize}
%\item $KB_1 \not\models {\pm}stm(poor,0)@0$;
%
%\item $KB_1 \models   {-}stm(poor,0)@1$; and
%
%\item $KB_1 \not\models {\pm}stm(poor,0)@2$. 
%\end{itemize}  

For the sake of our discussion, let us consider $KB_2 = \langle  O_1 , R_{1,}D_1 \cup \{prefer(d_2,d_1,[])\} \rangle$. 
It is easy to see that  every  answer set of $\Pi(KB_2)$ contains $defeated(d_1, \neg poor, 2)$. As such, we have that 
$KB_2 \not\models {\pm}stm(poor,0)@0$;
 $KB_2 \models   {-}stm(poor,0)@1$; and
 $KB_2  \models {+}stm(poor,0)@2$. 
 
%  \begin{itemize}
%\item $KB_2 \not\models {\pm}stm(poor,0)@0$;
%
%\item $KB_2 \models   {-}stm(poor,0)@1$; and
%
%\item $KB_2  \models {+}stm(poor,0)@2$. 
%\end{itemize}  


 
\subsection{Reasoning With Observations about Action Occurrences} 

In this section, we shift our focus to set $A$ of actions from the action signature $\langle F, B, A \rangle$. Following common practice in action languages \cite{GelfondL98}, we assume that each action is  associated with a set of literals, called its \emph{effects}, with each effect potentially predicated on a different set of literals, called \emph{preconditions}. A third set of literals, the \emph{executability conditions}, states when the action can be executed vs impossible. This information is encoded in statements of the form: 
$exec(a,body)$ and $causes(a,p,body)$ 
%\begin{align}
% exec(a,body) \label{exec} \\
% causes(a,p,body) \label{effect} 
%\end{align}
%
where $a \in A$ is an action, $p$ is a fluent literal, and $body$ is a set of fluent literals. 
 The first statement gives the executability conditions of action $a$, while the second states that, when $body$ holds, $p$ is one effect of $a$. We represent an \emph{action occurrence observation} (or {\em action occurrence}) by a statement of the form
 $occ(a,s)$
%\begin{equation}\label{occu}
%occ(a,s)
%\end{equation}
where $a\in A$ and $s$ is a time step. Recall that the notion of knowledge base already supports providing action occurrence observations, as  $H^{n}$ is a set of observations and action occurrences. The semantics of the above statements is encoded by suitable rules added to $\Pi(KB)$. 
 Note that the body of $exec$ and $causes$ statements is encoded by means of the $mbr$ predicate, as we did for rules and defaults in the previous section. 
We add to $\Pi(KB)$ the following rules:
 %and can be encoded   as follows. 
%
\begin{eqnarray}
obs(P, S)  & \leftarrow & s(S), occ(A, S), exec(A, M), \label{lp:exec} \\
& & mbr(P, M).  \nonumber    \\ 
obs(L, S+1) & \leftarrow & s(S),     occ(A, S), causes(A, L, M),\\
& &  N  = =\# count \{X : mbr(X, M)\},  \label{lp:effect} \nonumber\\
& & \# count \{X : mbr(X, M),  h(X, T) \} == N  \nonumber
\end{eqnarray} 

Rule \eqref{lp:exec} indicates that the observation of an action occurrence allows us to infer that its executability conditions must be satisfied---we capture this by generating new observations about the time step in which the action has occurred.
In rule \eqref{lp:effect}, we take the approach of encoding the effects of an action occurrence as observations; the rule allows us to observe the consequences of an action from knowledge of its occurrence.

%
%\begin{equation} \label{lp:exec} 
%\begin{array}{l}
%obs(L, S) {\leftarrow} s(S), 
%   occ(A, S), exec(A, M), 
%   mbr(L, M).    
%   \end{array}
%\end{equation}
%\begin{equation} \label{lp:effect}
%\begin{array}{l}
%obs(L, S+1) \leftarrow s(S),     
%   occ(A, S), causes(A, L, M), \\ 
%    \hspace{.5cm}  N  = \# count \{X : mbr(X, M)\},  \\ 
% \hspace{.5cm}  \# count \{X : mbr(X, M),  h(X, T) \} == N
% \end{array}
%\end{equation}   

% Representation of action theories
% 
% \begin{align}
% caused(\ell,body)\\
% causes(a,\ell,body)\\
% executable(a,body)
% \end{align}
%
%$Abusing the notation, we   use $\Pi(KB)$ to denote the program consisting of the facts representing $O$ and $RD$ 
%and the rules developed in the previous section and \eqref{lp:exec}-\eqref{lp:effect}. 
Definition~\ref{kb:entail} 
 is extended to knowledge bases with action occurrences in a trivial way.  

\begin{example}\label{ex3} 
%%Let us a
Assume that buying a house near John's university, in Example~\ref{ex1},
 requires an  amount of money that only wealthy people can afford. The effect and precondition of the action (denoted by $Act$) 
 can be represented by  
$cause(buy\_house, has\_house, [])$ and $exec(buy\_house, [\neg poor])$. 
Let us assume that we observe  John buying a house at time step $t_2$. 
Let $KB_3$ be $KB_1$ extended with $Act$ and the observation $occ(buy\_house,2)$. 
Given this specification, we  expect that the system tells us that the statement of John being poor made at step 0 is false at step 2. Indeed, 
this is the result sanctioned by $\Pi(KB_3)$, since rule \eqref{lp:exec} 
indicates that $\neg poor$ is observed at time 2 and, thus,  the default $d_2$ is defeated even when it is applicable (Rule $(11)$), i.e., $KB_3 \models {-}stm(\neg poor, 0)@2$ holds.
\end{example}  
 
% \iffalse 
% Rules to derive pre/post conditions from actions
% 
% Static should be the same as rules
% 
% Dynamic: if $a \causesif{\ell} \ell_1\wedge\dots\wedge\ell_n$:
%  \begin{itemize}
% \item If the action is present, its effects should hold:
%         \begin{align}
%         h(\ell,s+1) \leftarrow occ(a,s), h(\ell_1,s),\dots, h(\ell_n,s)
%         \end{align}
%  \item If the action is present then it should be executable; assuming
%          a single executable condition $a \executable \ell_1,\dots,\ell_n$, then
%          \begin{align}
%          h(\ell_i,s) \leftarrow occ(a,s) & & 1\leq i \leq n
%          \end{align}
%          
%  \item If the action is present we can try to infer its preconditions; for the
%  sake of simplicity, let us assume that for each action $a$ and literal $\ell$, there
%  is at most one dynamic causal law $a \causesif{\ell} \ell_1\wedge\dots\wedge\ell_n$. Then
%          \begin{align}
%          h(\ell_i,s) \leftarrow& occ(a,s), h(\ell,s+1), \\
%                                        &  not \:h(\bar{\ell_1},s), \dots, not\: h(\bar{\ell_n},s)
%          \end{align}
% \end{itemize} 
% 
% Use executability conditions as well?
% 
% Example
% 
% PRINCIPLES: (1) all actions are observed; 
% \fi
 
 
% \section{Observations and Statements with Preferences and Biases}

\if 0
\section{\textcolor{red}{[REMOVE]} Supporters and Denials} 

The previous section shows that we can compute the truthfulness of a statement given 
a set of observations and a set of defaults, preferences, and rules. {\em What should a user do 
when he/she cannot evaluate the truthfulness of a statement?}   
In this section, we answer this question by defining the notion of a \emph{supporter}
and \emph{denial} of a statement,
and discussing how to compute supporters/denials of a statement.
Intuitively, a supporter/denial will help an observer in determining the truthfulness of 
a given statement w.r.t. her knowledge base by identifying the set of 
observations that the observer needs to have. 


Given a $KB = \langle H^{n}, R,D \rangle$, we will use $now$ to denote a specific time step 
such that for every $obs(l, t)$ or $occ(a,t)$ in $H^{n}$, $now > t$. 
For a consistent set of literals $L$, let 
$OBS(L) = \{obs(l, now) \mid l \in L\}.$ 
The notions of supporters and denials are defined next. 

\begin{definition} 
Let $KB  = \langle H^{n}, R,D \rangle$ be a knowledge base.  Let $L$ be a consistent set of literals. 
  \begin{itemize}
\item  $L$ is said to be a \emph{supporter} of a belief literal $b$ w.r.t. $KB$  
if:  $\langle H^{n}  \cup OBS(L) , R,D \rangle \models {+}stm(b,0)@now$;  

\item $L$ is said to be a \emph{denial} of a belief literal $b$ w.r.t. $KB$ 
if: $\langle H^{n}  \cup OBS(L) , R,D \rangle\models {-}stm(b,0)@now$. 
\end{itemize}  
%where $KB \cup OBS(L) =  \langle O  \cup OBS(L) , RD \rangle$. 
%
$L$ is a \emph{minimal supporter (or denial)} of $l$  w.r.t. $KB$ if it is a subset minimal 
supporter (or denial) of $l$ w.r.t. $KB$.
\end{definition} 
%
The intuition behind the above definition is that a supporter (resp. denial) of a belief literal $b$ is a set of literals 
whose truth values need to be established before the truthfulness of a statement about $b$ can be determined. %Of course, 
Clearly, if an observer can observe every literal % in $\cal L$
then she would be able to determine the truthfulness of a statement about $b$. However, it is desirable to consider minimal supporters/denials. We next discuss possible ways to compute minimal supporters (resp. denials) of a statement about a literal $b$. 

\subsection{\textcolor{red}{[REMOVE]} ASP Computation}
 
%\subsection{Computing Minimal Supporters/Denials} 

A minimal supporter/denial of a belief literal $b$ w.r.t. $KB$ can be computed using $\Pi(KB)$ and a few extra rules that generate observations and evaluate the truth value of $l$ at the step $now$.

\begin{eqnarray}
 \{obs(P, now); obs(\neg P, now)\} 1 &\leftarrow  &   fluent(P).     \label{lp:generate} \\  
 nr\_obs(N) &\leftarrow &  N = \#count \{L:obs(L,now)\}.  \label{lp:count} \\ 
{\#minimize  \{N : nc\_obs(NC)\}.} & & \label{lp:min} 
\end{eqnarray}
%
Let:
\[
\begin{array}{lcl}
P(l) & = & \Pi(KB) \cup  \{\eqref{lp:generate}, \eqref{lp:count}, \eqref{lp:min}\} \cup \{{\leftarrow} \naf h(l, now).\}\\
N(l) &= &  \Pi(KB) \cup  \{\eqref{lp:generate}, \eqref{lp:count}, \eqref{lp:min}\} \cup \{{\leftarrow} \naf h(\overline{l}, now).\}
\end{array}
\]  
%
%
%\begin{align} 
%\leftarrow \naf h(l, now). \label{lp:supporter} 
%\end{align}   
%
%Let $N(l)$ be the program consisting of $\Pi(KB)$, the rules~\eqref{lp:generate}-\eqref{lp:min}, and the following rule: 
%\hfill{$\leftarrow \naf h(\overline{l}, now).$} 
%
%\begin{align} 
%\leftarrow \naf h(\overline{l}, now). \label{lp:denial} 
%\end{align}   
%
It is easy to see that the following holds: 
\begin{proposition} 
For a $KB = \langle O, R,D \rangle$ and a literal $l$, 
\begin{itemize} 
\item For every answer set $A$ of $P(l)$,  $L = \{l \mid obs(l, now) \in A\}$ is a minimal supporter of $l$ w.r.t. $KB$. 
\item For every answer set $A$ of $N(l)$,  $L = \{l \mid obs(l, now) \in A\}$ is a minimal denial of $l$ w.r.t. $KB$. 
\end{itemize} 
\end{proposition} 
Observe that $P(l)$ and $N(l)$ compute cardinality-minimal supporters and denials of $l$ w.r.t. $KB$. 
Since minimal-cardinality  implies subset minimality but the reverse is not true,  
$P(l)$ and $N(l)$ do not compute all minimal supporters/denials. We next explore a different method for computing {\em all} 
minimal supporters/denials.
%
%
\subsection{\textcolor{red}{[REMOVE]} CR-Prolog Computation}
%
%
We explore the use of CR-Prolog to address the issue of completeness in the computation of
the minimal supporters and denials.
CR-Prolog extends ASP by introducing an additional type of rules, called 
{\em consistency restoring rules} (or {\em cr-rules}), of the form  %
\begin{eqnarray} 
r: \:\:\:
c  \cra a_1,\ldots,a_m,\naf a_{m+1},\ldots,\naf a_n  \label{crrule} 
\end{eqnarray} %
where $r$ is the name of the rule and $c$  and $a_j$'s are 
literals. Observe that 
a cr-rule can be viewed as a normal rule by dropping its name 
and replacing the connective $\cra$ with $\leftarrow$. As such, we refer 
to $head(r)$, $pos(r)$, and $neg(r)$ for a cr-rule $r$ in the same way we refer to different elements of an
ASP rule.

A CR-program $P$ is given by a pair $(P^r,P^c)$ where $P^r$ is a 
set of ASP rules  and $P^c$ is a set
of rules of the form \eqref{crrule}. Let $C$ be a subset of $P^c$.
By $P^r \cup C$ we denote the program consisting of rules in $P^r$
and the cr-rules in $C$ viewed as normal rules. 

Answer sets of a CR-program $P$ are defined 
as follows. If $P^r$ is consistent, then any answer set of $P^r$ 
is an answer set of $P$. Otherwise, an answer set of $P$ is 
an answer set of $P^r \cup C$ where $C$ is a minimal subset
of $P^c$ such that $P^r \cup 
\{c \leftarrow body(r) \mid c \cra body(r) \in C\}$ 
is consistent. 

Let $\Pi^+(KB)$ (resp. $\Pi^-(KB)$) be the program consisting of $\Pi(KB)$, the rule 
$\leftarrow  \naf h(l, now)$ (resp. the rule $\leftarrow  \naf h(\overline{l}, now)$). 
Let $\Pi^C$ be  the set of cr-rules 
\begin{align} 
obs(P, now) \cra fluent(P). \label{lp:cra1} \\
obs(\neg P, now) \cra fluent(P). \label{lp:cra2} 
\end{align} 
We can show the following 
\begin{proposition} 
        For a $KB = \langle O, RD \rangle$ and a literal $l$, 
        \begin{itemize}
                \item A set of literals $L$  is a minimal supporter of $l$ with respect to $KB$ iff there exists an 
                answer set $A$ of $(\Pi^+(KB),\Pi^C)$ such that $L = \{l \mid obs(l, now) \in A\}$.
                
                \item A set of literals $L$  is a minimal denial of $l$ with respect to $KB$ iff there exists an 
                answer set $A$ of $(\Pi^-(KB),\Pi^C)$ such that $L = \{l \mid obs(l, now) \in A\}$.
                
        \end{itemize} 
\end{proposition}
\fi
 

\section{Applications} 

As a potential application of our framework, consider the following scenario. Bob is walking in a good area of the city when a person suddenly appears, brandishing a knife and displaying a menacing demeanor. At some point during the confrontation, Bob shoots and kills the person. A court needs to decide whether Bob acted in self-defense or not. Simplifying the scenario, we will assume that the only relevant law says that one acted in self-defense if they believed to be in imminent danger.

To make the example more interesting, suppose the police found that the knife the person was holding was fake. Thus, we can identify two possible scenarios:

    1. When Bob shot the person, he had not yet noticed that the knife was fake. We could expect to conclude that at that time Bob believed to be in imminent danger.

    2. If instead Bob shot the person after observing that the knife was fake, we could expect to conclude that, at the time of the shooting, Bob did not believe to be in imminent danger.

Assume that the police are able to tell when Bob shot the person and when he observed that the knife was fake, for example thanks to a reliable witness who saw the events from Bob's same perspective\footnote{In an actual case, other considerations may play a role, e.g. the reliability of the witness or whether the perspective was really the same. These complications are outside the scope of our example.}. We will demonstrate the use of our framework to reason about the evolution of Bob's beliefs and determine if he acted in self-defense.

A possible formalization of the knowledge base from this example is the following set $RD^s$ of statements:

\begin{lstlisting}[language=clingo,caption=Set $RD^s$ of statements for the self-defense example, label=lst:pi_S, mathescape=true,xleftmargin=.01\textwidth, breaklines=true]
causes(appears(X),is(X,present),[]).
default($d^s_1$,inImminentDanger,[has(att,k),
  is(att,present),is(att,menc)])
default($d^s_2$,$\neg$inImminentDanger,[has(att,k),
  is(att,present),is(att,menc),is(k,fake)])
prefer($d^s_2$,$d^s_1$, [ ]).
default($d^s_3$,$\neg$inImminentDanger,
  [at(a1), is(a1,good)])
prefer($d^s_1$,$d^s_3$, [ ]).
\end{lstlisting}
The representation uses fluents $has(p,o)$ (p has/holds object o), $is(o,a)$ (object o has attribute a), and $at(l)$ (Bob is at location $l$). The objects are $k$ (the knife), $a_1$ (the good area of the city), and $att$ (the attacker). Additionally, we attributes constants $present$ (for attribute ``physically present''), $fake$ (attribute of the knife), $menc$ (menacing-looking), and $good$ (attribute of the area of the city). The actions are $appears(p)$ (person $p$ appears) and $shoots(p)$ (Bob shoots $p$). The effect of the former is described by the $causes$ statement; the latter action has no effects within the scope of the example. The intuitive meaning of the defaults is:
\begin{itemize}
\item $d^s_1$: If there is someone who is menacing-looking and holding a knife, one will normally believe to be in imminent danger
\item $d^s_2$: In the above case, if the knife is fake, one will not normally believe to be in imminent danger
\item $d^s_3$: One does not normally believe to be in imminent danger in a good area of the city
\end{itemize} 
As can be seen from the $prefer$ statements, $d^s_2$ is the most preferred default and $d^s_3$ the least preferred.
The initial part of the story is captured by:
\[
\begin{array}{lll}
O^s_{init}  =  \left\{
\begin{array}{ll} 
obs(at(a_1),0).   %\ \ \ \ 
& obs(is(a_1,good),0). \\
obs(\neg is(att,present),0). & 
hpd(appears(att),0). \\    % the guy appears
obs(is(att,menc),1). &  %\ \ \ \     % the guy is menacing-looking
obs(has(att,k),1).
\end{array}
\right\}
\end{array}
\]
Let us consider what our framework yields for $KB^s_{init}=\langle O^s_{init},RD^s \rangle$. One can check that the answer set of $\Pi(KB^s_{init})$ contains $h(at(a_1),0)$ and $h(is(a_1,good),0)$. The only applicable default is $d^s_3$, yielding $believes(\neg inImminentDanger,0)$, i.e. Bob does not believe to be in imminent danger at time step $0$. The two possible evolutions of the story are formalized as follows:
\begin{itemize}
\item Scenario 1: \[
\begin{array}{lll}
O^s_{1} & = & \left\{
\begin{array}{l} 
hpd(shoots(bob,att),1). 
\end{array}
\right\}\\
\ \\
\multicolumn{3}{l}{KB^s_1=\langle O^S_{init} \cup O^s_1,RD^s \rangle}
\end{array}
\]
The time step of interest is $1$. Note that $h(is(att,menc),1)$ and $h(has(att,k),1)$ follow trivially from the observations. Defaults $d^s_1$ and $d^s_3$ are thus applicable, while $d^{s}_2$ is not.  Because $d^s_1$ is preferred to $d^s_3$, the answer set of $\Pi(KB^s_1)$ contains $believes(inImminentDanger, 1)$, meaning that, at that time Bob pulled the trigger, he believed to be in imminent danger at the time of the shooting. It is worth noting how the framework is able to capture the evolution of Bob's beliefs over time.  
\item Scenario 2: \[
\begin{array}{lll}
O^s_{2} & = & \left\{
\begin{array}{l} 
obs(is(k,fake),1). \\   % the knife is fake
hpd(shoots(bob,att),1).
\end{array}
\right\} \\
\ \\
\multicolumn{3}{l}{KB^s_2=\langle O^S_{init} \cup O^s_2,RD^s \rangle}
\end{array}
\]
At time step $1$, Bob has observed that the knife is fake and, thus, $h(is(k,fake),1)$ holds. Hence, all defaults are applicable.  Because $d^s_2$ is preferred to $d^s_1$, the answer set of $\Pi(KB^s_2)$ contains $believes(\neg inImminentDanger, 1)$. That is, Bob did not initially believe to be in imminent danger and did not believe so at the time of the shooting either. 
\end{itemize}

% application.tex

For another demonstration of our framework, consider the problem of detecting \emph{Man-in-the-Middle} (MITM) attacks targeting computer and cyber-physical systems.  In a MITM attack, the attacker secretly places itself as an intermediary between two communicating parties, relaying the information between them. By intercepting the communications, the attacker may steal valuable information, or even alter the information exchanged between the parties and fool them into performing unintended or undesirable actions. 
For example, a MITM attack was used in Stuxnet,\footnote{\tiny \url{https://en.wikipedia.org/wiki/Stuxnet}} a sophisticated malicious software (malware) that targeted certain models of industrial Programmable Logic Controllers (PLCs). Stuxnet is remarkable in that it is reported to have been successful in impacting industrial systems involved in Iran's nuclear enrichment program. Its success has major implications on the security  of industrial systems world-wide.

%
\begin{figwindow}[3,r,%
        {\includegraphics[width=.5\textwidth]{mim}},%
        {MITM attack carried out by Stuxnet: safe commands sent by the PLC (green) are replaced by dangerous ones (red); alarm readings from the centrifuge's sensors (red) are replaced by  safe ones\label{fig:stuxnet}}]
\end{figwindow}
The behavior of Stuxnet's MITM component is outlined in Figure \ref{fig:stuxnet}. Its component operates by intercepting the commands sent by a PLC to a connected centrifuge. The malware first increased the speed of the centrifuge above normal levels for a short amount of time, and later slowed it down below normal levels for a longer period of time. It is believed that the resulting stress caused components of the centrifuge to expand and eventually destroy it. Under normal conditions, sensors installed in the centrifuge would have alerted the PLC---and its users---about the abnormal conditions, giving them a chance to shut down the system before damage occurred. However, as part of the MITM attack, Stuxnet also intercepted the sensor readings from the centrifuge, and sent to the PLC fake readings based on previous recordings that indicated that the system was operating normally (this is known as a \emph{replay attack}). 

A seriously concerning feature of MITM attacks is their ability to take control of the involved parties' inputs and outputs, making the attack virtually undetectable to the parties. In this section, we show that detection of a MITM attack is indeed possible if the detection task is reduced to that of reasoning about the truthfulness of a communication partner, as long as one has access to some external knowledge that can be used 
to evaluate
%as supporter or denial of 
the partner's statements.

To demonstrate this, we consider a simplified\ MITM attack scenario along the lines of Stuxnet's MITM component. For simplicity of presentation, we do not include in the scenario occurrences of actions, but it is not difficult to see that our approach extends in a natural way when actions occur.  %are present


Consider the case of a motor, $M$, that can be \emph{on} or \emph{off.} A sensor mounted on the motor tells whether the motor is overheating. A controller, $C$, is programmed to turn off the motor if it is found to be overheating.

Suppose now that the system is the target of a MITM attack. An attacker, $A$, manages to place itself between $C$ and $M$, intercepting the communications between them. $A$ intercepts the output of $M$'s sensor, discarding the sensor reading and always providing $C$ with a negative reading independently of the actual state of $M$, i.e, %. In doing so, 
the attacker could prevent $C$ from turning off an overheating motor, eventually causing it to be %come 
damaged. How can such an attack be detected?

The solution leverages a technique for reasoning about cyber-physical systems and their interaction with the physical environment discussed in \cite{nb15}. Suppose that a thermostat, $T$ is  located near the motor,  which generates an audible alert every $20$ minutes if the temperature of the room is above $90^\circ$F, in order to ensure that the operators take more frequent breaks. Based on world knowledge, during cold weather an observer will normally believe the temperature in the room to be below $90^\circ$F.
 

The thermostat is not related to the functioning of the motor and, thus, it is conceivable that $A$ will not attempt to alter its activities.
However, given the proximity of the motor, an alert from the thermostat when the room is not hot can be taken as an indication that $M$ is indeed overheating (and that $M$'s sensor reading may have been tampered with). Let us see how one can draw this conclusion using our framework.

The relevant information can be formalized as shown below. The signature is defined so that $alert$ and $winter$ are fluents, while all others are beliefs. %using the following statements:
\iffalse
{
\[
\begin{array}{l}
default(d_1^m, hot\_room, [alert]). \\
default(d_2^m, \neg hot\_room, [cold\_weather]). \\
prefer(d_2^m,d_1^m, [ \: ]).\\
rule(r_1^m,cold\_weather,[winter]). \\
default(d_3^m,overheat,[alert, \neg hot\_room]).
\end{array}
\]
}
\fi
%
\begin{lstlisting}[language=clingo,caption=Relevant Information, label=lst:pi_M, mathescape=true,xleftmargin=.01\textwidth, breaklines=true]
default($d_1^m$, hot_room, [alert]). 
default($d_2^m$, $\neg$hot_room, [cold_weather]). 
prefer($d_2^m$,$d_1^m$, [ ]). 
rule($r_1^m$,cold_weather,[winter]). 
default($d_3^m$,overheat,[alert, $\neg$hot_room]).
\end{lstlisting}



Default $d_1^m$ says that, as a rule, an observer hearing an alert from $T$ will believe the room to be hot. Default $d_2^m$ states that the observer will not believe the room to be hot during cold weather. The third statement expresses a preference for $d_2^m$ when both $d_1^m$ and $d_2^m$ are applicable. This captures the intuition that, during cold weather,  one will have a tendency to assume that the room is not hot even if an alert is heard. Rule $r_1^m$ states that, during the winter, one will believe the weather to be cold\footnote{We encode this statement in a non-defeasible way for the purpose of illustrating the $rule$ construct. In a more realistic scenario, one may want to use a default instead.}. The final statement says that, typically, if an alert is generated by $T$ when the room is not hot, then the observer will believe that $M$ is overheating. The statement is encoded as a default to increase elaboration tolerance, making it possible, for example, to take into account faults in the thermostat or the presence of other heat sources.

Let us now suppose that we would like to evaluate the truthfulness of $M$'s sensor reading, and suppose that the sensor reports that there is no overheating. The corresponding statement is:
$stm(\neg overheat,0).$
Next, an alert is generated by $T$:
$
obs(alert,1).
$
Let $KB_m$ be the corresponding knowledge base. Clearly, default $d_2^m$ is not applicable. Default $d_1^m$ leads the reasoner to conclude $believes(hot\_room,1)$, that is, that the temperature in the room is above $90^\circ$F. Thus, the reasoner has no reason to doubt the sensor reading:
$
KB_{m} \models {+}stm(\neg overheat,0)@1.
$

Next, the system is informed that it is winter. The updated knowledge base, $KB_m'$, extends $KB_m$ by the statement
$
obs(winter,2).
$
It is not difficult to check that the answer set of $\Pi(KB_m')$ contains $believes(cold\_weather,2)$ because of the application of rule $r^m_1$. Both defaults $d^m_1$ and $d^m_2$ are now applicable, but because of the preference over them, only the latter is applied, yielding the conclusion $believes(\neg hot\_room,2)$. Our framework now derives:
$
KB_m' \models {-}stm(\neg overheat,0)@2.
$
That is, the sensor reading from $M$ is deemed not truthful, which indicates that the system may be under a MITM attack.

\iffalse
\textcolor{red}{[REMOVE]} Finally, one can also reason about supporters and denials of $\neg overheat$. Given $KB_m$ and the above fluents, for example, it is not difficult to show that   $\emptyset$ is the minimal supporter  of $\neg overheat$ and $\{ winter \}$ is its minimal denial.
\fi 
   
\section{Related Work }

%\memo{expand -- in addition with trust modeling, lp with preferences, default logic with preferences, cr-prolog} 

Within the scope of the ASP and logic programming community, this work is, to the best of our knowledge,  novel. As we have mentioned earlier, there is an extensive literature---within and outside the ASP community---for reasoning about actions and change, reasoning about defaults, and diagnostic reasoning about observations. Besides action languages used in this paper, event calculus (e.g., \cite{KowalskiS86,den92a}) or situation calculus (e.g., \cite{McCarthyH69,Reiter01}) are formalisms for reasoning about actions and change and could be used instead of action languages; and, these formalisms have also been used in diagnosis (e.g., \cite{AriasCCG19,mci-th}). All of these works are somewhat related to the proposed system. However, they are separate formalisms/systems that can be used as components of our system. In this sense, our work is related to    \cite{BalducciniG03b,BalducciniG03} and the ones mentioned earlier, such as \cite{brew99,BrewkaE00,DelgrandeST03,GelfondS98} as they are using ASP  in the implementation of a certain formalism. Also of note is that the proposed framework bears  similarity  to frameworks developed to
support \emph{diagnosis} as both rely on observations to draw conclusions but our framework 
does not focus on explaining what goes wrong and does not assume completeness of the 
knowledge of the agent (e.g., in the form of a complete
model).   

Formalisms like ASP with preferences (e.g., \cite{Brewka05,BrewkaDRS15})  can provide a foundation for the implementation of the proposed framework---e.g., by 
facilitating the preferences between defaults.

%The work by \cite{BalducciniG03b} is related somewhat to our proposed implementation but only deals with observations and its extension \cite{BalducciniG03} includes an action theory but neither deals with a theory of defaults.     They are similar examples of separate formalisms for reasoning about observations or reasoning with defaults as mentioned earlier. 

The AI community has explored the issue of computational trust and reputation in several works---please see \cite{SabaterS05} for a survey. The survey focuses predominantly on trust models observed in multi-agent scenarios, taking explicitly into account the observations concerning interactions among agents. The survey provides classification of the models according to different dimensions: conceptual model (cognitive vs. game theoretical), information sources (direct interactions, direct observations, witness information, sociological information, prejudice), visibility (subjective vs. global), granularity (context dependent vs. non-context dependent), model type (trust vs. reputation), type of information exchanged (boolean vs. continuous), and agent behavior's assumptions (honest, biased but not lying, lying). Within such classification, our model focuses on trust---but could easily accommodate other interesting forms of reputation)---based on cognitive aspects, it builds on direct observations, but can accommodate sociological biases and prejudice through defaults, it captures subjective visibility, it is context dependent and relies on boolean information.
 
The survey in~\cite{ArtzG07} places a greater emphasis on surveying models of trust as models to predict attitude towards future interactions with an agent---with less emphasis on assessing the trustworthiness of a current statement.  

Finally, this paper substantially expands and improves upon our prior work \cite{spg16,spg16b}. Specifically, in this paper:
%\begin{itemize}
%\item
we develop a clearer characterization of the reasoning task of interest and of the role played by the agents involved;
%\item
we provide a more precise treatment of, and distinction between, fluents and beliefs;
%\item
we develop a characterization of the evolution of beliefs as a stand-alone, central reasoning task;
%\item
we redefine the task of reasoning about the truthfulness of statements;
%\item
we develop a more rigorous abstract framework, including a clear description of the various forms of entailment involved and of their roles;
%\item
we include proofs of the main claims; and
%\item
we include new and expanded applications and use cases to highlight the new findings.
%\end{itemize}

%\memo{need to work on -- to the conclusion 
%}
  
\iffalse  
 
\subsection{Relaxing Consequence} 
One of the advantages of using a declarative language like ASP is the ability
to explore alternative reasoning strategies. The proposed encoding is skeptical in 
the way it handles the assessment of statements---by considering a statement true when
supported by \emph{all} answer sets and false when its negation is supported by
all answer sets. There are variations of this approach that could be easily modeled. For example,
a trusting observe may want to accept the statement of the agent as long as this is not
explicitly contradicted.

\begin{definition}
Let $KB = \langle O, RD\rangle$ be a knowledge base and let us consider a statement 
$stm(l,s)$. We say that:
\begin{itemize}

\item $KB \models {+}stm(l,s)@t$ if for every answer set $A$ of $KB$ we have that
        $h(\bar{l},t)\not\in A$;
\item $KB \models {-}stm(l,s)@t$ if every answer set $A$ of $KB$ contains
        $h(\bar{l},t)$;
\item the statement is otherwise unknown.
\end{itemize} 
\end{definition}
This definition, applied to Example~\ref{ex2}, allows us to conclude:
  $KB_1 \models {+}stm(poor,0)@0$;
 $KB_1 \models   {-}stm(poor,0)@1$; and
 $KB_1 \models {+}stm(poor,0)@2$.
%
%\begin{itemize} 
%\item $KB_1 \models {+}stm(poor,0)@0$;
%
%\item $KB_1 \models   {-}stm(poor,0)@1$; and
%
%\item $KB_1 \models {+}stm(poor,0)@2$. 
%\end{itemize}  
   
\memo{
talk about time annotated knowledge bases, 
reputation evaluation, diagnosis} 
   
  
         
\subsection{Time-Annotated Knowledge Bases} 


The components of a knowledge base---as defined in the previous section---relate properties of the world at a single time instance. It is easy to imagine situations in which rules, defaults, and preferences could relate to information in different time points. For example, if the unit of time is months, the information ``if a graduate student fails the final examination, he/she needs to wait at least 1 year before re-taking the exam'' cannot be represented as a rule in the current framework since the relation between the fluent $take\_exam\_second\_time$ and the fluent $fail\_exam\_first\_time$  spans 12 units of times. 

We will next propose a generalization of the proposed framework, called 
\emph{time-annotated knowledge bases}, for dealing with the above issue. Let $l$ be a literal and $t$ be an integer. $l^t$ is called a \emph{time-annotated literal}. A \emph{time-annotated} rule, default, or preference has the form \eqref{rule}, \eqref{default}, or \eqref{pref}, respectively, whose $head$ and $body$ are now time-annotated literals and sets of time-annotated literals. For example, the relation between the two fluents $take\_exam\_second\_time$ ($ts$) and $fail\_exam\_first\_time$ ($ff$) can be represented by the set of rules: 
$$
\left \{ 
rule(exame\_time, (\neg ts)^i,  [ff^0])  
\mid i=1,\ldots,11 
\right\}
$$
 
Given a time-annotated knowledge base $KB$, the program $\Pi(KB)$ defined in the previous section can be easily modified for computing the truthfulness of statements. In fact, in all the rules of $\Pi(KB)$, $h(L, S)$ is replaced with $h(U, S+T)$ if $L$ is the time-annotated literal $U^S$. As an example, the rule~\eqref{lp:rule} becomes 

  \begin{equation}\label{lp:rule:tt}
\begin{array}{l}
h(H, S+T) \leftarrow  s(S),  
        rule(D, H^T, M),  \\ 
     \hspace{.1cm} N = \# count \{L^T : mbr(M, L^T)\},  \\ 
    \hspace{.1cm}  \# count \{L^T : mbr(M, L^T),  h(L, T{+}S) \} {==} N. 
      \end{array}
\end{equation} 


 
% \subsection{  Rules and Defaults} 

%Extending the language to allow time-dependent rules and defaults. Introducing time annotated literals: $l^t$ to denote $h(l, S+t)$. 
%
%\begin{align}
%rule(r, head^t, body) 
%\end{align} 

 
\subsection{A System for Reputation Evaluation}    
Given that reputation of agents is exhibited on what they say and what they do, the proposed framework can be used for evaluating agents' reputation if we have a rule (or a set of rules) for the evaluation, i.e., what does it mean for an agent to have excellent, good, or bad reputation. A reasonable rule for such a purpose could be devised using the ratio between the truthful and non-truthful statements made by agents, e.g., an agent has an excellent reputation if the ratio between her  truthful and non-truthful statements is 99\%. We note that this percentage is likely dependent on individual observer, the agent, and the application. For instance, it might be okay to classify a saleman as excellent if the percentage is 70\% but this thresthold would be insufficient for a doctor to be called as excellent. 

Given a knowledge base $KB$, a set of statements $S$ made by an agent, and a classification rule of the form ``an agent is of the type $xType$ if the ratio  between her  truthful and non-truthful statements is $y\%$,'' the program $\Pi_Q(KB)$ can be extended for determining whether or not the agent is of the type $xType$ in the following way. 

\begin{align} 
reputation(xType, T) \leftarrow s(T),\quad\quad  \label{rep} \\
NT = \#count \{L,T : \mathtt{t}(L, T)\},  \:\: \nonumber \\
NF =  \#count \{L,T : \mathtt{f}(L, T)\}, \nonumber \\
NT > 0, NT*100  \ge NF*y.        \quad\quad \:\: \nonumber
\end{align}     
where we assume that $y$ is given by an integer ranging between 0 and 100. 

Observe that this rule determines the reputation of the agent at the step $T$. The rule could be changed to compute the reputation of an agent over the full history considered by the program. Using the same method for computing the truthfulness of agents' statements, we can 
answer the question of whether or not the agent is of the type $xType$. 

Let $\Pi_R  = \Pi_Q  \cup \{\eqref{rep}\}$. If the program  
$\Pi_R   \cup \{\leftarrow \naf reputation(xType, t)\}$ has an answer set and the program 
$\Pi_R(KB)  \cup \{\leftarrow reputation(xType, t)\}$ has no answer set then we can conclude that the agent is of the type $xType$ 
at the time step $t$. 

\fi

%\memo{Discussing on different ways to deal with statement -- using information from other agents in evaluating truthfulness of other statements} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse 
\begin{itemize} 
\item a \emph{conditional default} is of the form 
\begin{align}\label{c:default}
\alpha \Rightarrow default(d, head, body)
\end{align} 
where $d$ is the name of the default, $head$ is a fluent literal, and $body$ is a collection of literals.  A default \eqref{default} says that normally, if $body$ holds then $head$ also holds. 
\item a \emph{conditional preference} between two defaults is expressed by 
\begin{align}\label{c:prefs}
\alpha \Rightarrow prefer(d_1, d_2)
\end{align} 
where $d_1$ and $d_2$ are two default names. This says that $d_1$ is more preferred than $d_2$. 

\end{itemize} 

\subsection{Dealing with Inertial} 
 Rules for reasoning with inertial:
%
\begin{align}
ab\_inertial(H, \overline{H}, S) \leftarrow s(S), \label{lp:notinertial} \\
        app(D_1, H, S), \nonumber \\
        app(D_2, \overline{H}, S), \nonumber \\
        \naf defeated(D_1, H, S),  \nonumber \\
        \naf defeated(D_2, \overline{H}, S). \nonumber \\
h(H, S+1) \leftarrow s(S), h(H, S), \label{lp:inertial} \\ 
        \naf h(\overline{H}, S), \nonumber \\
        \naf ab\_inertial(H, \overline{H}, S). \nonumber        
\end{align} 
In the above rules, $ab\_inertial(H, \overline{H}, S)$ is an atom that indicates that 
The first rule, \eqref{lp:notinertial}, encodes our skepticism by defining the atom $not\_inertial(H, \overline{H}, S)$ w


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Final Remark}
Let us conclude with a final remark. 
 The proposed framework bears  similarity  to the ASP frameworks developed to
 support \emph{diagnosis}. In particular, both frameworks  focus on using observations (and an underlying logical theory) to infer the correctness of some entity---a statement in our framework, a system in the case of diagnosis. 
 On the other hand, there is a profound difference between the two frameworks: unlike diagnosis, our 
 framework does  not rely on  a well elaborated theory of how the agent works. In particular, our frameworks 
 does not make any assumption about completeness of knowledge of the agent (e.g., in the form of a complete
 model).
  \fi 
   
 \section{Conclusions and Future Work}
In this paper, we proposed a general framework for reasoning about the truthfulness of statements
made by an agent. We showed how the framework can be implemented using ASP using well-known methodologies for reasoning about actions and change and for default reasoning with preferences. The framework does not assume complete knowledge about the agent being observed and the reasoning 
process builds on observations about the state of the world and occurrences of actions. We explored the use of the
framework in simple scenarios derived from man-in-the-middle attacks and  placed the proposed framework in the context of other related work.


The proposed work can be extended in several directions. First, the default theory considered in this paper is static in the sense that its rules and defaults encode knowledge about the world at a single time point. It is not difficult to imagine that rules and defaults can related knowledge at different time points (e.g., the knowledge about the behavior of some machine within a time interval). 
Second, we note that our implementation is rather impartial to statements by others (the ({\bf SK}) principle). An alternative view of this is to believe in every statement unless it is proven false. Third, it is reasonable to assume that there might be several sources of information that a user can employ to collect observations which include the agents that are observed. The reliability of each source might influence our decision on whether or not a statement could be believed as true. We believe that a combination of a model of trust together with attitude of agents as described in \cite{SabaterS05,ArtzG07} and our framework might be useful here. 
Our  system can be easily extended to deal with the first extension 
as it currently deals with time stamped literals already. The second extension might require the change in Def.~\ref{kb:entail}. The third extension might involve a significant amount of modification. We leave all these extensions for future work.  


\section*{Acknowledgment}

This research is partially supported by NSF grants 1757207, 1812619, 1812628, and 1914635. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring organizations, agencies, or the U.S. government.


 
 

%\bibliographystyle{named}
\bibliographystyle{kr}
\bibliography{enrico,bibfile,bib2010,bib-mbal}

\end{document} 
